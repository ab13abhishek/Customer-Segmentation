{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y_5eQ4CSAcI"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "#  Standard Library\n",
        "# ===============================\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Suppress TensorFlow logs\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=INFO, 2=WARNING, 3=ERROR\n",
        "\n",
        "\n",
        "# ===============================\n",
        "#  Data Handling\n",
        "# ===============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ===============================\n",
        "#  Visualization\n",
        "# ===============================\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "import umap\n",
        "import plotly.io as pio\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Suppress specific warnings from seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "#  Machine Learning & Clustering\n",
        "# ===============================\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from sklearn.cluster import (\n",
        "    KMeans,\n",
        "    MiniBatchKMeans,\n",
        "    Birch,\n",
        "    DBSCAN,\n",
        "    AgglomerativeClustering,\n",
        "    AffinityPropagation,\n",
        "    MeanShift,\n",
        "    estimate_bandwidth,\n",
        "    SpectralClustering,\n",
        "    OPTICS,\n",
        "    estimate_bandwidth\n",
        ")\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "# ===============================\n",
        "#  Metrics & Evaluation\n",
        "# ===============================\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score\n",
        ")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "#  Hierarchical Clustering\n",
        "# ===============================\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "\n",
        "\n",
        "# ===============================\n",
        "#  Utilities\n",
        "# ===============================\n",
        "!pip install kneed\n",
        "from kneed import KneeLocator\n",
        "import hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYKqxPX8_QBy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('online_retail.csv', encoding='ISO-8859-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a34e0e0"
      },
      "source": [
        "# Task 1\n",
        "Inspect the dataframe `df` by displaying the first few rows, column names, data types, and summary statistics. Check for and handle missing and duplicate values. Visualize the data distribution and relationships between variables. Summarize the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dc5e28b"
      },
      "source": [
        "## Initial data inspection\n",
        "\n",
        "Display the first few rows, column names, data types, and summary statistics of the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ff66f40"
      },
      "outputs": [],
      "source": [
        "display(df.head())\n",
        "display(df.info())\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ecbd8f9"
      },
      "source": [
        "## Handle missing values\n",
        "\n",
        "Check for missing values and decide on an appropriate strategy to handle them (e.g., imputation or removal).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e96e9101"
      },
      "outputs": [],
      "source": [
        "missing_values = df.isnull().sum()\n",
        "display(\"Missing values in each column:\")\n",
        "display(missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c150020"
      },
      "outputs": [],
      "source": [
        "df_cleaned = df.dropna(subset=['Description', 'Customer ID'])\n",
        "missing_values_after_drop = df_cleaned.isnull().sum()\n",
        "display(\"Missing values after dropping rows with missing 'Description' or 'Customer ID':\")\n",
        "display(missing_values_after_drop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1b9c5a"
      },
      "source": [
        "## Handle duplicate values\n",
        "\n",
        "Check for duplicate values and decide on an appropriate strategy to handle them (e.g., removal).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d087eba4"
      },
      "outputs": [],
      "source": [
        "duplicate_rows = df_cleaned.duplicated().sum()\n",
        "display(f\"Number of duplicate rows: {duplicate_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61b795fc"
      },
      "outputs": [],
      "source": [
        "df_cleaned = df_cleaned.drop_duplicates(keep='first')\n",
        "duplicate_rows_after_drop = df_cleaned.duplicated().sum()\n",
        "display(f\"Number of duplicate rows after removal: {duplicate_rows_after_drop}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "857364b4"
      },
      "source": [
        "## Data visualization\n",
        "\n",
        "Generate relevant visualizations to understand the data distribution and relationships between variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42f02091"
      },
      "outputs": [],
      "source": [
        "# Set global style\n",
        "sns.set_theme(style=\"whitegrid\", font_scale=0.9)\n",
        "\n",
        "# Pre-compute frequently used values\n",
        "top_countries = df_cleaned['Country'].value_counts().nlargest(10)\n",
        "top_stock_codes = df_cleaned.groupby('StockCode')['Quantity'].sum().nlargest(10)\n",
        "sample_df = df_cleaned.sample(n=10000, random_state=42)\n",
        "\n",
        "# Function to rotate and align x-tick labels\n",
        "def rotate_xlabels(ax, rotation=90, ha='right'): # Increased rotation to 90\n",
        "    ax.set_xticks(ax.get_xticks()) # Use FixedLocator by getting current ticks\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), ha=ha)\n",
        "\n",
        "# Subplots for Quantity, Price, and Total_Price Histograms\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "fig.suptitle('Distribution of Data', fontsize=16)\n",
        "\n",
        "# Scatter: Quantity vs Price\n",
        "sns.scatterplot(x='Quantity', y='Price', data=sample_df, ax=axes[0])\n",
        "axes[0].set(title='Quantity vs Price (Sampled)', xlabel='Quantity', ylabel='Price', xlim=(-100, 200), ylim=(-10, 100))\n",
        "\n",
        "# Bar: Top 10 Countries\n",
        "sns.barplot(\n",
        "    x=top_countries.index,\n",
        "    y=top_countries.values,\n",
        "    hue=top_countries.index,\n",
        "    palette=\"viridis\",\n",
        "    legend=False,\n",
        "    ax=axes[1]\n",
        ")\n",
        "axes[1].set(title='Top 10 Countries by Number of Orders', xlabel='Country', ylabel='Number of Orders')\n",
        "plt.setp(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Bar: Top 10 Stock Codes\n",
        "sns.barplot(\n",
        "    x=top_stock_codes.index,\n",
        "    y=top_stock_codes.values,\n",
        "    hue=top_stock_codes.index,\n",
        "    palette=\"mako\",\n",
        "    legend=False,\n",
        "    ax=axes[2]\n",
        ")\n",
        "axes[2].set(title='Top 10 Most Sold Stock Codes by Quantity', xlabel='Stock Code', ylabel='Total Quantity Sold')\n",
        "plt.setp(axes[2].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33961e71"
      },
      "source": [
        "# Task 2\n",
        "Analyze and visualize the dataframe `df`. Perform initial data inspection, handle missing values (excluding 'Customer ID'), handle duplicate values, and analyze/merge rows based on 'Invoice', 'StockCode', and 'Customer ID'. Finally, visualize the data and summarize the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "112836e1"
      },
      "source": [
        "## Analyze and potentially merge rows\n",
        "\n",
        "Group the data by (Invoice, StockCode, Customer ID) and analyze the groups. Decide on a strategy to handle groups with multiple rows (e.g., merging).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b29a48b"
      },
      "outputs": [],
      "source": [
        "grouped = df_cleaned.groupby(['Invoice', 'StockCode', 'Customer ID']).size().reset_index(name='row_count')\n",
        "multiple_entries = grouped[grouped['row_count'] > 1]\n",
        "display(\"Groups with multiple entries:\")\n",
        "display(multiple_entries.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcc13a0e"
      },
      "outputs": [],
      "source": [
        "df_aggregated = df_cleaned.groupby(['Invoice', 'StockCode', 'Description', 'Customer ID', 'Country', 'InvoiceDate']).agg(\n",
        "    Quantity=('Quantity', 'sum'),\n",
        "    Price=('Price', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# Verify that there are no more multiple entries for the same key after aggregation\n",
        "grouped_aggregated = df_aggregated.groupby(['Invoice', 'StockCode', 'Customer ID']).size().reset_index(name='row_count')\n",
        "multiple_entries_after_aggregation = grouped_aggregated[grouped_aggregated['row_count'] > 1]\n",
        "display(\"Groups with multiple entries after aggregation:\")\n",
        "display(multiple_entries_after_aggregation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6980ff28"
      },
      "outputs": [],
      "source": [
        "df_aggregated = df_aggregated.drop_duplicates(subset=['Invoice', 'StockCode', 'Customer ID'], keep='first')\n",
        "\n",
        "# Final verification\n",
        "grouped_final = df_aggregated.groupby(['Invoice', 'StockCode', 'Customer ID']).size().reset_index(name='row_count')\n",
        "multiple_entries_final = grouped_final[grouped_final['row_count'] > 1]\n",
        "display(\"Groups with multiple entries after final deduplication:\")\n",
        "display(multiple_entries_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTggYhu8h5HL"
      },
      "outputs": [],
      "source": [
        "df_aggregated.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4da2bbd1"
      },
      "outputs": [],
      "source": [
        "def cleaner(df):\n",
        "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='mixed', dayfirst=True)\n",
        "    print(\"\\n--- Cleaning: Description and StockCode ---\")\n",
        "    for col in ['Description', 'StockCode']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.strip().str.lower()\n",
        "            print(f\"'{col}' column cleaned (stripped and lowercased).\")\n",
        "        else:\n",
        "            print(f\"'{col}' column not found — skipped.\")\n",
        "    return df\n",
        "\n",
        "dff = cleaner(df_aggregated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff31b9cc"
      },
      "source": [
        "## Disambiguate Transaction Types\n",
        "\n",
        "This code defines a function `disambiguate_transaction_types` that classifies transactions into different types ('Sale', 'Internal Adjustment', 'Customer Return') based on the characteristics of the 'Quantity', 'Invoice', and 'Description' columns. It helps in separating sales transactions from returns or internal adjustments for more accurate analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0IEUD0ogxXd"
      },
      "outputs": [],
      "source": [
        "def disambiguate_transaction_types(df):\n",
        "    # Lower case invoice and description\n",
        "    if 'Invoice' in df.columns:\n",
        "        df['Invoice_cleaned'] = df['Invoice'].astype(str).str.upper().str.strip()\n",
        "    else:\n",
        "        raise KeyError(\"Column 'Invoice' is missing from the DataFrame.\")\n",
        "\n",
        "    if 'Description' in df.columns:\n",
        "        df['Description_cleaned'] = df['Description'].astype(str).str.lower().str.strip()\n",
        "    else:\n",
        "        raise KeyError(\"Column 'Description' is missing from the DataFrame.\")\n",
        "\n",
        "    # Internal adjustment phrases\n",
        "    internal_adjustment_phrases = [\n",
        "        'damages', 'samples/damages', 'damages/display', 'damages?', 'damages/showroom etc',\n",
        "        'damages/credits from asos.', 'damages/dotcom?', 'damages/samples', 'damages wax',\n",
        "        '????damages????', 'possible damages or lost?', 'incorrectly made-thrown away.',\n",
        "        'wrong barcode', 'wrong barcode (22467)', 'sold with wrong barcode',\n",
        "        'wrongly sold as sets', 'wrongly sold sets', 'wrongly sold (22719) barcode',\n",
        "        'check', 'stock check', 'check?', '??', '???', '???lost', '?? missing',\n",
        "        '????missing', '???missing', 'lost??', '???',\n",
        "        'wet damaged', 'reverse 21/5/10 adjustment', 'adjustment', 'adjust',\n",
        "        'reverse previous adjustment', 'adjust bad debt', 'taig adjust', 'taig adjust no stock',\n",
        "        'temp adjustment', 'oops ! adjustment', 'dotcom adjust', 'missing', '?missing',\n",
        "        'missing?', 'faulty', 're-adjustment'\n",
        "    ]\n",
        "\n",
        "    # Default all to 'Sale'\n",
        "    df['Transaction_Type'] = 'Sale'\n",
        "\n",
        "    # Internal Adjustments\n",
        "    df.loc[\n",
        "        (df['Quantity'] < 0) &\n",
        "        (~df['Invoice_cleaned'].str.startswith('C')) &\n",
        "        (df['Description_cleaned'].isin(internal_adjustment_phrases)),\n",
        "        'Transaction_Type'\n",
        "    ] = 'Internal Adjustment'\n",
        "\n",
        "    # Customer Returns\n",
        "    df.loc[\n",
        "        (df['Quantity'] < 0) &\n",
        "        (df['Invoice_cleaned'].str.startswith('C')),\n",
        "        'Transaction_Type'\n",
        "    ] = 'Customer Return'\n",
        "\n",
        "    # 6: Summary\n",
        "    print(\"Transaction types classified:\")\n",
        "    print(df['Transaction_Type'].value_counts().to_dict())\n",
        "    df.drop(columns=['Invoice_cleaned', 'Description_cleaned'], axis=1, inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ1r_poFhJMM"
      },
      "outputs": [],
      "source": [
        "dff = disambiguate_transaction_types(dff)\n",
        "dff.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKpNRAkKn2SL"
      },
      "source": [
        "## Categorize Item Types\n",
        "\n",
        "This code defines a function `categorize_item_types` that assigns each transaction to a specific **item type** (e.g., *Product, Discount, Shipping/Service, Amazon Related, Operational Adjustment*) based on rules applied to the `StockCode` and `Description` fields.\n",
        "\n",
        "It starts by assuming all items are *Products* and then reclassifies them using keyword matching (such as \"postage\" → *Shipping/Service*, \"discount\" → *Discount*, \"amazon fee\" → *Amazon Related*). It also applies **StockCode-based overrides** for special cases (`'D'` → *Discount*, `'M'` → *Manual Adjustment*).\n",
        "\n",
        "Additionally, operational anomalies (e.g., damages, missing items, wrong barcodes) are grouped as *Operational Adjustments*. For transactions marked as *Customer Returns*, the function further refines classifications into *Discount Reversal* or *Shipping Refund* where applicable.\n",
        "\n",
        "This categorization helps distinguish between normal product sales and **non-standard adjustments/fees**, improving downstream analysis of sales, profitability, and operational issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v1CI-uXhdEO"
      },
      "outputs": [],
      "source": [
        "def categorize_item_types(df):\n",
        "    # Default all to 'Product'\n",
        "    df['Item_Type'] = 'Product'\n",
        "    df['Description'] = df['Description'].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # Define keyword lists\n",
        "    service_keywords = ['postage', 'dotcom postage']\n",
        "    discount_keywords = ['discount']\n",
        "    manual_keywords = ['manual']\n",
        "    amazon_keywords = [\n",
        "        'amazon', 'amazon fee', 'amazon sales', 'amazon sold sets',\n",
        "        'sold as set on dotcom and amazon', 'amazon adjustment', 'amazon adjust'\n",
        "    ]\n",
        "    operational_keywords = [\n",
        "        'damages', 'samples/damages', 'damages/display', 'damages?', 'damages/showroom etc',\n",
        "        'damages/credits from asos.', 'damages/dotcom?', 'damages/samples', 'damages wax',\n",
        "        '????damages????', 'possible damages or lost?', 'incorrectly made-thrown away.',\n",
        "        'wrong barcode', 'wrong barcode (22467)', 'sold with wrong barcode',\n",
        "        'wrongly sold as sets', 'wrongly sold sets', 'wrongly sold (22719) barcode',\n",
        "        'stock check', 'check?', '??', '???', '???lost', '?? missing',\n",
        "        '????missing', '???missing', 'lost??',\n",
        "        'wet damaged', 'reverse 21/5/10 adjustment', 'adjustment', 'adjust',\n",
        "        'reverse previous adjustment', 'adjust bad debt', 'taig adjust', 'taig adjust no stock',\n",
        "        'temp adjustment', 'oops ! adjustment', 'dotcom adjust', 'missing', '?missing',\n",
        "        'missing?', 'faulty', 're-adjustment'\n",
        "    ]\n",
        "\n",
        "    # StockCode rules\n",
        "    stockcode_rules = {\n",
        "        'd': 'Discount',\n",
        "        'm': 'Manual Adjustment',\n",
        "        'post': 'Shipping/Service',\n",
        "        'amazon': 'Amazon Related'\n",
        "    }\n",
        "    for code, item_type in stockcode_rules.items():\n",
        "        mask = df['StockCode'].astype(str).str.lower() == code.lower()\n",
        "        df.loc[mask, 'Item_Type'] = item_type\n",
        "\n",
        "    # Description-based rules\n",
        "    description_rules = {\n",
        "        'Shipping/Service': service_keywords,\n",
        "        'Discount': discount_keywords,\n",
        "        'Manual Adjustment': manual_keywords,\n",
        "        'Amazon Related': amazon_keywords\n",
        "    }\n",
        "    for item_type, keywords in description_rules.items():\n",
        "        pattern = r'\\b(?:' + '|'.join(re.escape(k) for k in keywords) + r')\\b'\n",
        "        mask = df['Description'].str.contains(pattern, na=False, regex=True)\n",
        "        df.loc[mask & (df['Item_Type'] == 'Product'), 'Item_Type'] = item_type\n",
        "\n",
        "    # Operational keywords with word boundaries to avoid false positives like \"pink check\"\n",
        "    operational_pattern = r'\\b(?:' + '|'.join(re.escape(k) for k in operational_keywords) + r')\\b'\n",
        "    operational_mask = df['Description'].str.contains(operational_pattern, na=False, regex=True)\n",
        "    df.loc[operational_mask & (df['Item_Type'] == 'Product'), 'Item_Type'] = 'Operational Adjustment'\n",
        "\n",
        "    # Special handling for returns\n",
        "    return_mask = df['Transaction_Type'] == 'Customer Return'\n",
        "    df.loc[return_mask & (df['Item_Type'] == 'Discount'), 'Item_Type'] = 'Discount Reversal'\n",
        "    df.loc[return_mask & (df['Item_Type'] == 'Shipping/Service'), 'Item_Type'] = 'Shipping Refund'\n",
        "\n",
        "    # Summary\n",
        "    print(\"Item type distribution:\")\n",
        "    print(df['Item_Type'].value_counts().to_string())\n",
        "\n",
        "    return df\n",
        "dff =  categorize_item_types(dff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOz-2BYuoMiy"
      },
      "source": [
        "## Handle Zero and Anomalous Prices\n",
        "\n",
        "`handle_zero_and_anomalous_prices` cleans the `Price` column by converting values to numeric, dropping invalid entries, and removing *Product* rows with zero or negative prices. This ensures only valid prices remain for accurate sales analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN0U4dHJhplN"
      },
      "outputs": [],
      "source": [
        "def handle_zero_and_anomalous_prices(df):\n",
        "    # Convert Price to numeric\n",
        "    df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
        "\n",
        "    # Drop rows with invalid (non-numeric) Price\n",
        "    df.dropna(subset=['Price'], inplace=True)\n",
        "\n",
        "    # Identify 'Product' rows with non-positive Price\n",
        "    mask_invalid_price = (df['Item_Type'] == 'Product') & (df['Price'] <= 0)\n",
        "    deleted_rows = df[mask_invalid_price].copy()\n",
        "\n",
        "    # Remove them from the main DataFrame\n",
        "    df = df[~mask_invalid_price].copy()\n",
        "\n",
        "    print(f\"Removed {len(deleted_rows)} 'Product' rows with non-positive prices.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "dff = handle_zero_and_anomalous_prices(dff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu-PEWE1pyTx"
      },
      "source": [
        "## Extract Time Features\n",
        "\n",
        "`extract_time_features` converts `InvoiceDate` to datetime, drops invalid entries, and derives new time-based features (`Year`, `Month`, `Day`, `DayOfWeek`, `Hour`). These features enable temporal analysis of sales trends and customer behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCTxdMFUpqea"
      },
      "outputs": [],
      "source": [
        "def extract_time_features(df):\n",
        "    # Convert 'InvoiceDate' to datetime\n",
        "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
        "    initial_rows = len(df)\n",
        "\n",
        "    # Drop rows where InvoiceDate conversion failed\n",
        "    df.dropna(subset=['InvoiceDate'], inplace=True)\n",
        "    dropped = initial_rows - len(df)\n",
        "\n",
        "    if dropped > 0:\n",
        "        print(f\"Dropped {dropped} rows due to invalid 'InvoiceDate'.\")\n",
        "\n",
        "    # Create time-based features\n",
        "    df['InvoiceYear'] = df['InvoiceDate'].dt.year\n",
        "    df['InvoiceMonth'] = df['InvoiceDate'].dt.month\n",
        "    df['InvoiceDay'] = df['InvoiceDate'].dt.day\n",
        "    df['InvoiceDayOfWeek'] = df['InvoiceDate'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "    df['InvoiceHour'] = df['InvoiceDate'].dt.hour\n",
        "\n",
        "    print(\"Time-based features (Year, Month, Day, DayOfWeek, Hour) extracted.\")\n",
        "    return df\n",
        "dff = extract_time_features(dff)\n",
        "dff.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LNgBV2_opyu"
      },
      "source": [
        "## Sales Summary\n",
        "\n",
        "`sales_summary` generates a post-cleaning overview of sales data. It reports key metrics such as total transactions, invoices, customers, date range, average items per invoice, gross/net revenue, and unique products. It also highlights the top 5 countries by sales and shows distributions of transaction and item types for quick business insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUnRSmq6mnk-"
      },
      "outputs": [],
      "source": [
        "## Creating Total Price column\n",
        "dff.loc[:, 'Total_Price'] = dff['Quantity'] * dff['Price']\n",
        "print(\"'Total_Price' column calculated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3zysHARlMhN"
      },
      "outputs": [],
      "source": [
        "def sales_summary(df):\n",
        "    # Filter for actual product sales\n",
        "    sales_df = df[(df['Transaction_Type'] == 'Sale') & (df['Item_Type'] == 'Product')]\n",
        "    returns_df = df[(df['Transaction_Type'] == 'Customer Return') & (df['Item_Type'] == 'Product')]\n",
        "\n",
        "    # Core metrics\n",
        "    total_transactions = len(df)\n",
        "    unique_invoices = df['Invoice'].nunique()\n",
        "    unique_customers = df['Customer ID'].nunique()\n",
        "    date_range_start = df['InvoiceDate'].min()\n",
        "    date_range_end = df['InvoiceDate'].max()\n",
        "    average_items_per_invoice = df.groupby('Invoice')['Quantity'].sum().mean()\n",
        "\n",
        "    # Revenue calculations\n",
        "    total_gross_revenue = sales_df['Total_Price'].sum()\n",
        "    total_net_revenue = total_gross_revenue + returns_df['Total_Price'].sum()\n",
        "\n",
        "    # Other insights\n",
        "    top_countries_by_sales = sales_df.groupby('Country')['Total_Price'].sum().nlargest(5)\n",
        "    unique_products = sales_df['StockCode'].nunique()\n",
        "\n",
        "    # Output\n",
        "    print(f\"Total transactions: {total_transactions}\")\n",
        "    print(f\"Unique invoices: {unique_invoices}\")\n",
        "    print(f\"Unique customers: {unique_customers}\")\n",
        "    print(f\"Date range: {date_range_start.date()} to {date_range_end.date()}\")\n",
        "    print(f\"Average items per invoice: {average_items_per_invoice:.2f}\")\n",
        "    print(f\"Total gross revenue (products only): Rs. {total_gross_revenue:,.2f}\")\n",
        "    print(f\"Total net revenue (sales - returns): Rs. {total_net_revenue:,.2f}\")\n",
        "    print(f\"Number of unique products sold: {unique_products}\")\n",
        "\n",
        "    print(\"\\nTop 5 countries by product sales:\")\n",
        "    print(top_countries_by_sales)\n",
        "\n",
        "    print(\"\\nTransaction_Type distribution:\")\n",
        "    print(df['Transaction_Type'].value_counts())\n",
        "\n",
        "    print(\"\\nItem_Type distribution:\")\n",
        "    print(df['Item_Type'].value_counts())\n",
        "sales_summary(dff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YntAzPy9m0bN"
      },
      "outputs": [],
      "source": [
        "dff.to_csv('cleaned_data.csv', index=False)\n",
        "data = dff.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ox4mXcvIK6e"
      },
      "source": [
        "# Task 3\n",
        "\n",
        "Split the dataset by country and focus on a selected country (e.g., India) for detailed analysis. Visualize the country-specific data, analyze product prices, and categorize them into ranges (low, medium, high). Separate transactions into sales and returns while also analyzing sales across price segments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ikTd3h3y3WM"
      },
      "source": [
        "## Country-wise Data Split\n",
        "\n",
        "This code groups the dataset by `Country`, creating a dictionary of DataFrames for each. It prints the number of transactions per country, then builds a summary showing row/column counts, average rows, and identifies the countries with the most and fewest transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4imfjpx_m-YV"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of DataFrames by country\n",
        "dfs_by_country = {country: data for country, data in data.groupby('Country')}\n",
        "\n",
        "# Print shapes for all countries\n",
        "print(\"\\nNumber of transactions by country:\")\n",
        "print(\"-\" * 40)\n",
        "for country, df in dfs_by_country.items():\n",
        "    print(f\"{country.ljust(20)}: {len(df):,} rows\")\n",
        "\n",
        "# Summary statistics\n",
        "country_stats = pd.DataFrame({\n",
        "    'Country': dfs_by_country.keys(),\n",
        "    'Rows': [df.shape[0] for df in dfs_by_country.values()],\n",
        "    'Columns': [df.shape[1] for df in dfs_by_country.values()]\n",
        "})\n",
        "\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total countries: {len(dfs_by_country)}\")\n",
        "print(f\"Average rows per country: {country_stats['Rows'].mean():,.0f}\")\n",
        "print(f\"Country with most transactions: {country_stats.loc[country_stats['Rows'].idxmax(), 'Country']}\")\n",
        "print(f\"Country with fewest transactions: {country_stats.loc[country_stats['Rows'].idxmin(), 'Country']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhbA9EUxpUVr"
      },
      "source": [
        "## Visualize Country Data - INDIA\n",
        "\n",
        "`visualize_country_data` generates a dashboard of plots for a given country, showing transaction and item type distributions, top products (by quantity and revenue), top customers, monthly sales trends, hourly and weekday sales patterns, and a correlation heatmap of key numeric fields. It provides a comprehensive visual overview of country-level sales behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVNdYUvznOd1"
      },
      "outputs": [],
      "source": [
        "# Access the DataFrame for 'India'\n",
        "df_india = dfs_by_country['India']\n",
        "df_india.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlKxL9mgnT-d"
      },
      "outputs": [],
      "source": [
        "def visualize_country_data(df_country, country_name):\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "    fig, axes = plt.subplots(4, 2, figsize=(18, 20))\n",
        "    fig.suptitle(f\"Data Overview for {country_name}\", fontsize=20, fontweight=\"bold\")\n",
        "\n",
        "    # Transaction Type Distribution\n",
        "    df_country['Transaction_Type'].value_counts().plot(\n",
        "        kind='pie', autopct='%1.1f%%', ax=axes[0,0], startangle=90, colors=sns.color_palette(\"pastel\"))\n",
        "    axes[0,0].set_title(\"Transaction Type Distribution\")\n",
        "    axes[0,0].set_ylabel(\"\")\n",
        "\n",
        "    # Item Type Distribution\n",
        "    df_country['Item_Type'].value_counts().plot(\n",
        "        kind='pie', autopct='%1.1f%%', ax=axes[0,1], startangle=90, colors=sns.color_palette(\"pastel\"))\n",
        "    axes[0,1].set_title(\"Item Type Distribution\")\n",
        "    axes[0,1].set_ylabel(\"\")\n",
        "\n",
        "    # Top Selling Products by Quantity\n",
        "    top_products_quantity = df_country.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
        "    top_products_quantity.plot(kind='bar', ax=axes[1,0], color='skyblue')\n",
        "    axes[1,0].set_title(\"Top 10 Products by Quantity Sold\")\n",
        "    axes[1,0].set_ylabel(\"Quantity\")\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "    axes[1,0].set_xticklabels(axes[1,0].get_xticklabels(), ha='right')\n",
        "\n",
        "    # Top Selling Products by Revenue\n",
        "    top_products_revenue = df_country.groupby('Description')['Total_Price'].sum().sort_values(ascending=False).head(10)\n",
        "    top_products_revenue.plot(kind='bar', ax=axes[1,1], color='salmon')\n",
        "    axes[1,1].set_title(\"Top 10 Products by Revenue\")\n",
        "    axes[1,1].set_ylabel(\"Revenue\")\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    axes[1,1].set_xticklabels(axes[1,1].get_xticklabels(), ha='right')\n",
        "\n",
        "    # Top Customers by Spend\n",
        "    top_customers = df_country.groupby('Customer ID')['Total_Price'].sum().sort_values(ascending=False).head(10)\n",
        "    top_customers.plot(kind='bar', ax=axes[2,0], color='orange')\n",
        "    axes[2,0].set_title(\"Top 10 Customers by Spend\")\n",
        "    axes[2,0].set_ylabel(\"Total Spend\")\n",
        "    axes[2,0].tick_params(axis='x', rotation=45)\n",
        "    axes[2,0].set_xticklabels(axes[2,0].get_xticklabels(), ha='right')\n",
        "\n",
        "    # Monthly Sales Trend\n",
        "    sales_trend = df_country.set_index('InvoiceDate').resample('ME')['Total_Price'].sum()\n",
        "    sales_trend.plot(ax=axes[2,1], marker='o', color='green')\n",
        "    axes[2,1].set_title(\"Monthly Sales Trend\")\n",
        "    axes[2,1].set_ylabel(\"Total Spend\")\n",
        "    axes[2,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Sales by Hour of Day\n",
        "    sales_by_hour = df_country.groupby('InvoiceHour')['Total_Price'].sum()\n",
        "    sales_by_hour.plot(kind='bar', ax=axes[3,0], color='purple')\n",
        "    axes[3,0].set_title(\"Sales by Hour of Day\")\n",
        "    axes[3,0].set_xlabel(\"Hour of Day\")\n",
        "    axes[3,0].set_ylabel(\"Total Spend\")\n",
        "    axes[3,0].tick_params(axis='x', rotation=0)\n",
        "\n",
        "    # Sales by Day of Week\n",
        "    sales_by_dayofweek = df_country.groupby('InvoiceDayOfWeek')['Total_Price'].sum()\n",
        "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "    sales_by_dayofweek.index = sales_by_dayofweek.index.map(lambda x: days[x])\n",
        "    sales_by_dayofweek.plot(kind='bar', ax=axes[3,1], color='brown')\n",
        "    axes[3,1].set_title(\"Sales by Day of Week\")\n",
        "    axes[3,1].set_xlabel(\"Day of Week\")\n",
        "    axes[3,1].set_ylabel(\"Total Spend\")\n",
        "    axes[3,1].tick_params(axis='x', rotation=45)\n",
        "    axes[3,1].set_xticklabels(axes[3,1].get_xticklabels(), ha='right')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
        "    plt.show()\n",
        "\n",
        "visualize_country_data(df_india, \"India\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ywFSf3gAmfx"
      },
      "source": [
        "## Analyze Prices\n",
        "\n",
        "`analyze_prices` generates side-by-side visualizations of a numeric column (default `Total_Price`), using a boxplot to highlight outliers and spread, and a violin plot to show the underlying distribution and density. It provides a quick yet comprehensive view of data dispersion, skewness, and anomalies, helping identify unusual price patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5749f19"
      },
      "outputs": [],
      "source": [
        "def analyze_prices(df, col='Total_Price'):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Boxplot (IQR and outliers)\n",
        "    sns.boxplot(x=df[col], ax=ax1, color='skyblue')\n",
        "    ax1.set_title(f'Boxplot of {col}')\n",
        "\n",
        "    # Violinplot (distribution density)\n",
        "    sns.violinplot(x=df[col], ax=ax2, color='lightgreen')\n",
        "    ax2.set_title(f'Distribution of {col}')\n",
        "\n",
        "    plt.show()  # Prevents auto-display\n",
        "# 1. Run analysis (no plots shown yet)\n",
        "analyze_prices(df_india)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6lVXXAiBfM5"
      },
      "source": [
        "## Categorize Price Ranges\n",
        "\n",
        "`categorize_price_ranges` bins a numeric column (default `Total_Price`) into predefined intervals such as `< -1000`, `-1000 to -100`, `-100 to 0`, `0 to 100`, `100 to 1000`, and `> 1000`. It adds a new categorical column (`Price_Range`) to the DataFrame for easier segmentation, and produces a summary table of counts per range. This dual output allows both **row-level categorization** and **aggregated insights** into how records are distributed across price bands, making it easier to detect trends, outliers, and concentration zones in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "255379d9"
      },
      "outputs": [],
      "source": [
        "def categorize_price_ranges(df, price_col=\"Total_Price\", country=\"India\"):\n",
        "    # Make a copy to avoid modifying original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Define price ranges\n",
        "    price_bins = [-float('inf'), -1000, -100, 0, 100, 1000, float('inf')]\n",
        "    price_labels = ['< -1000', '-1000 to -100', '-100 to 0',\n",
        "                    '0 to 100', '100 to 1000', '> 1000']\n",
        "\n",
        "    # Categorize into bins\n",
        "    df_copy['Price_Range'] = pd.cut(\n",
        "        df_copy[price_col],\n",
        "        bins=price_bins,\n",
        "        labels=price_labels,\n",
        "        right=False  # left-inclusive, right-exclusive\n",
        "    )\n",
        "\n",
        "    # Count distribution\n",
        "    price_range_counts = (\n",
        "        df_copy[price_col]\n",
        "        .groupby(pd.cut(df_copy[price_col],\n",
        "                        bins=price_bins,\n",
        "                        labels=price_labels,\n",
        "                        right=False),\n",
        "                 observed=False)\n",
        "        .size()\n",
        "        .reset_index(name=\"Count\")\n",
        "        .rename(columns={price_col: \"Price Range\"})\n",
        "    )\n",
        "\n",
        "    print(f\"Distribution of {price_col} for {country} by Range:\")\n",
        "    display(price_range_counts)\n",
        "\n",
        "    print(f\"\\nDataFrame with Price_Range column for {country}:\")\n",
        "    display(df_copy.head())\n",
        "\n",
        "    return df_copy\n",
        "df_india = categorize_price_ranges(df_india)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-7TzZH7DBiP"
      },
      "source": [
        "## Separate Sales, Returns, and Price Segments\n",
        "\n",
        "`separate_sales_and_returns` partitions the dataset into **sales** and **returns**, further isolating transactions involving only `Product` items. It returns four DataFrames: product-level sales, all sales, product-level returns, and all returns, while printing their shapes for verification.\n",
        "\n",
        "When combined with **price segmentation**, the function enables focused analysis within specific price brackets (e.g., `< -1000`, `-1000 to -100`, `-100 to 0`, `0 to 100`, `100 to 1000`, `> 1000`). Analysts can zoom into subsets like low-value (`0–100`) or extreme negative values (`< -1000`) to compare **sales vs. returns patterns**, **anomalies**, and **profitability** across ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxcdXvC_uaTh"
      },
      "outputs": [],
      "source": [
        "def separate_sales_and_returns(df):\n",
        "    # Separate sales and returns\n",
        "    sales_df = df[df['Transaction_Type'] == 'Sale'].copy()\n",
        "    returns_df = df[df['Transaction_Type'] == 'Customer Return'].copy()\n",
        "\n",
        "    # Filter only 'Product' items from sales\n",
        "    sales_df_products = sales_df[sales_df['Item_Type'] == 'Product'].copy()\n",
        "    returns_df_products = returns_df[returns_df['Item_Type'] == 'Product'].copy()\n",
        "\n",
        "\n",
        "    # Print shapes for confirmation\n",
        "    print(f\"\\nSales transactions (products only) shape: {sales_df_products.shape}\")\n",
        "    print(f\"Sales transactions (all) shape: {sales_df.shape}\")\n",
        "    print(f\"Return transactions (products only) shape: {returns_df_products.shape}\")\n",
        "    print(f\"Return transactions (all) shape: {returns_df.shape}\")\n",
        "\n",
        "    return sales_df_products, sales_df, returns_df_products, returns_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxluwvk14oQH"
      },
      "outputs": [],
      "source": [
        "# price_labels = ['< -1000', '-1000 to -100', '-100 to 0', '0 to 100', '100 to 1000', '> 1000']\n",
        "df_less_than_minus_1000 = df_india[df_india['Price_Range'] == '< -1000']\n",
        "df_minus_1000_to_minus_100 = df_india[df_india['Price_Range'] == '-1000 to -100']\n",
        "df_minus_100_to_0 = df_india[df_india['Price_Range'] == '-100 to 0']\n",
        "df_0_to_100 = df_india[df_india['Price_Range'] == '0 to 100']\n",
        "df_100_to_1000 = df_india[df_india['Price_Range'] == '100 to 1000']\n",
        "df_greater_than_1000 = df_india[df_india['Price_Range'] == '> 1000']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwpxvwW653UW"
      },
      "outputs": [],
      "source": [
        "sales_df_products, sales_df, returns_df_products, returns_df = separate_sales_and_returns(df_0_to_100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcIPE9ITukP3"
      },
      "outputs": [],
      "source": [
        "analyze_prices(sales_df_products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI4BAMy3OBOj"
      },
      "source": [
        "# Task 4\n",
        "\n",
        "Perform advanced analysis by conducting RFM (Recency, Frequency, Monetary) analysis for customers. Carry out cohort analysis to study customer retention over time. Analyze overall sales performance and trends across different time periods. Evaluate the impact of returns and refunds on both sales and customer value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiF3JmyMH0Jq"
      },
      "source": [
        "## RFM Analysis\n",
        "\n",
        "`rfm` computes **Recency, Frequency, and Monetary (RFM) scores** for each customer to evaluate their purchase behavior.\n",
        "\n",
        "* **Recency** measures how recently a customer purchased.\n",
        "* **Frequency** captures how often they purchased.\n",
        "* **Monetary** reflects the total spend.\n",
        "\n",
        "The function assigns **R, F, and M scores (1–5)** using quintiles, then combines them into an **RFM code** and segments customers into categories such as *Champions*, *Loyal Customers*, *Potential Loyalists*, *At Risk*, and *Lost*.\n",
        "\n",
        "It returns:\n",
        "\n",
        "1. **`rfm_df`** → detailed RFM scores and assigned segment for each customer.\n",
        "2. **`rfm_segment_profiles`** → aggregated segment profiles showing average metrics, number of customers, and contribution to total revenue (both absolute and percentage).\n",
        "\n",
        "This analysis provides a structured way to identify high-value customers, retention risks, and growth opportunities, making it a cornerstone for customer segmentation and targeted marketing strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hitrmpekKcgx"
      },
      "outputs": [],
      "source": [
        "def rfm(df):\n",
        "    # Ensure datetime format\n",
        "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "    # Snapshot date = 1 day after the last purchase\n",
        "    snapshot_date = df['InvoiceDate'].max() + timedelta(days=1)\n",
        "\n",
        "    # Calculate Recency, Frequency, Monetary\n",
        "    rfm_df = df.groupby('Customer ID').agg(\n",
        "        Recency=('InvoiceDate', lambda x: (snapshot_date - x.max()).days),\n",
        "        Frequency=('Invoice', 'nunique'),\n",
        "        Monetary=('Total_Price', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Filter out invalid customers\n",
        "    rfm_df = rfm_df[(rfm_df['Frequency'] > 0) & (rfm_df['Monetary'] > 0)]\n",
        "\n",
        "    # Scoring (5 = best for Recency, 5 = best for Frequency/Monetary)\n",
        "    rfm_df['R_Score'] = pd.qcut(rfm_df['Recency'].rank(method='first'), 5, labels=[5,4,3,2,1])\n",
        "    rfm_df['F_Score'] = pd.qcut(rfm_df['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
        "    rfm_df['M_Score'] = pd.qcut(rfm_df['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
        "\n",
        "    # Combine into single RFM code\n",
        "    rfm_df['RFM_Score'] = rfm_df['R_Score'].astype(str) + \\\n",
        "                          rfm_df['F_Score'].astype(str) + \\\n",
        "                          rfm_df['M_Score'].astype(str)\n",
        "\n",
        "    # Segment customers\n",
        "    def rfm_segment(row):\n",
        "        r, f, m = int(row['R_Score']), int(row['F_Score']), int(row['M_Score'])\n",
        "        if r == 5 and f == 5 and m == 5:\n",
        "            return 'Champions'\n",
        "        elif f >= 4 and m >= 4:\n",
        "            return 'Loyal Customers'\n",
        "        elif r >= 4 and f >= 3:\n",
        "            return 'Potential Loyalists'\n",
        "        elif r == 5 and f == 1:\n",
        "            return 'New Customers'\n",
        "        elif r <= 2 and f >= 3:\n",
        "            return 'At Risk'\n",
        "        elif r <= 2 and f <= 2:\n",
        "            return 'Lost'\n",
        "        else:\n",
        "            return 'Others'\n",
        "\n",
        "    rfm_df['Segment'] = rfm_df.apply(rfm_segment, axis=1)\n",
        "\n",
        "\n",
        "    # Aggregate segment profiles\n",
        "    rfm_segment_profiles = rfm_df.groupby('Segment').agg(\n",
        "        Avg_Recency=('Recency', 'mean'),\n",
        "        Avg_Frequency=('Frequency', 'mean'),\n",
        "        Avg_Monetary=('Monetary', 'mean'),\n",
        "        Num_Customers=('Customer ID', 'count')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Add percentages\n",
        "    rfm_segment_profiles['Percentage_of_Customers'] = (\n",
        "        rfm_segment_profiles['Num_Customers'] / rfm_df.shape[0] * 100\n",
        "    ).round(2)\n",
        "\n",
        "    total_revenue = rfm_df['Monetary'].sum()\n",
        "    rfm_segment_profiles['Percentage_of_Revenue'] = (\n",
        "        rfm_segment_profiles['Avg_Monetary'] * rfm_segment_profiles['Num_Customers'] / total_revenue * 100\n",
        "    ).round(2)\n",
        "\n",
        "\n",
        "    return rfm_df, rfm_segment_profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqQzTfzIKjxy"
      },
      "outputs": [],
      "source": [
        "rfm_table, rfm_profiles = rfm(sales_df_products)\n",
        "\n",
        "# Print preview\n",
        "display(\"Sample RFM data with segments:\")\n",
        "display(rfm_table.head())\n",
        "\n",
        "# Print profiles\n",
        "display(\"RFM Segment Profiles:\")\n",
        "display(rfm_profiles.sort_values(by='Percentage_of_Revenue', ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ada5d4"
      },
      "outputs": [],
      "source": [
        "def plot_rfm_profiles(rfm_profiles):\n",
        "  # Sort profiles by Percentage_of_Revenue for better visualization\n",
        "  rfm_profiles_sorted = rfm_profiles.sort_values(by='Percentage_of_Revenue', ascending=False)\n",
        "\n",
        "  fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "  fig.suptitle('RFM Segment Profiles Visualization', fontsize=16)\n",
        "\n",
        "  # Plot 1: Percentage of Customers by Segment\n",
        "  sns.barplot(x='Segment', y='Percentage_of_Customers', data=rfm_profiles_sorted, ax=axes[0, 0])\n",
        "  axes[0, 0].set_title('Percentage of Customers by RFM Segment')\n",
        "  axes[0, 0].set_ylabel('Percentage (%)')\n",
        "  axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "  plt.setp(axes[0, 0].get_xticklabels(), ha='right')\n",
        "\n",
        "  # Plot 2: Percentage of Revenue by Segment\n",
        "  sns.barplot(x='Segment', y='Percentage_of_Revenue', data=rfm_profiles_sorted, ax=axes[0, 1])\n",
        "  axes[0, 1].set_title('Percentage of Revenue by RFM Segment')\n",
        "  axes[0, 1].set_ylabel('Percentage (%)')\n",
        "  axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "  plt.setp(axes[0, 1].get_xticklabels(), ha='right')\n",
        "\n",
        "  # Plot 3: Average Monetary Value by Segment\n",
        "  sns.barplot(x='Segment', y='Avg_Monetary', data=rfm_profiles_sorted, ax=axes[1, 0])\n",
        "  axes[1, 0].set_title('Average Monetary Value by RFM Segment')\n",
        "  axes[1, 0].set_ylabel('Average Monetary Value')\n",
        "  axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "  plt.setp(axes[1, 0].get_xticklabels(), ha='right')\n",
        "\n",
        "  # Plot 4: Average Frequency by Segment\n",
        "  sns.barplot(x='Segment', y='Avg_Frequency', data=rfm_profiles_sorted, ax=axes[1, 1])\n",
        "  axes[1, 1].set_title('Average Frequency by RFM Segment')\n",
        "  axes[1, 1].set_ylabel('Average Frequency')\n",
        "  axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "  plt.setp(axes[1, 1].get_xticklabels(), ha='right')\n",
        "\n",
        "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "  plt.show()\n",
        "\n",
        "plot_rfm_profiles(rfm_profiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWfYJBbPJ6_d"
      },
      "source": [
        "## Cohort Analysis\n",
        "\n",
        "`cohort_analysis` performs customer cohort analysis by grouping customers based on their first purchase month and tracking their retention and spending behavior over time. It generates two heatmaps: one showing retention rates (% of customers who return in subsequent months) and another showing average revenue per active customer within each cohort. This helps businesses measure customer loyalty, monitor churn patterns, and understand the long-term value of different cohorts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8iP8hcwQih4"
      },
      "outputs": [],
      "source": [
        "def cohort_analysis(df, date_col='InvoiceDate', customer_col='Customer ID', value_col='Total_Price'):\n",
        "    cohort_df = df.copy()\n",
        "    cohort_df[date_col] = pd.to_datetime(cohort_df[date_col])\n",
        "\n",
        "    # Extract Invoice Month\n",
        "    cohort_df['InvoiceMonth'] = cohort_df[date_col].dt.to_period('M')\n",
        "\n",
        "    # First purchase month for each customer\n",
        "    customer_first_purchase = cohort_df.groupby(customer_col)['InvoiceMonth'].min().reset_index()\n",
        "    customer_first_purchase.columns = [customer_col, 'CohortMonth']\n",
        "\n",
        "    # Merge cohort month into main DF\n",
        "    cohort_df = pd.merge(cohort_df, customer_first_purchase, on=customer_col)\n",
        "\n",
        "    # Cohort index\n",
        "    def get_cohort_index(row):\n",
        "        year_diff = row['InvoiceMonth'].year - row['CohortMonth'].year\n",
        "        month_diff = row['InvoiceMonth'].month - row['CohortMonth'].month\n",
        "        return year_diff * 12 + month_diff\n",
        "\n",
        "    cohort_df['CohortIndex'] = cohort_df.apply(get_cohort_index, axis=1)\n",
        "\n",
        "    # Retention matrix\n",
        "    cohort_counts = cohort_df.groupby(['CohortMonth', 'CohortIndex'])[customer_col].nunique().reset_index()\n",
        "    cohort_counts.rename(columns={customer_col: 'TotalCustomers'}, inplace=True)\n",
        "    cohort_pivot = cohort_counts.pivot(index='CohortMonth', columns='CohortIndex', values='TotalCustomers')\n",
        "    cohort_sizes = cohort_pivot.iloc[:, 0]\n",
        "    retention_matrix = cohort_pivot.divide(cohort_sizes, axis=0) * 100\n",
        "\n",
        "    # Revenue per active customer\n",
        "    cohort_revenue = cohort_df.groupby(['CohortMonth', 'CohortIndex'])[value_col].sum().reset_index()\n",
        "    cohort_avg_revenue = cohort_revenue.pivot(index='CohortMonth', columns='CohortIndex', values=value_col)\n",
        "    cohort_avg_revenue = cohort_avg_revenue.divide(cohort_pivot, axis=0)\n",
        "\n",
        "    # --- Combined subplot figure ---\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(26, 10))\n",
        "\n",
        "    # Plot 1: Retention\n",
        "    sns.heatmap(retention_matrix, annot=True, fmt='.1f', cmap='YlGnBu', linewidths=.5,\n",
        "                cbar_kws={'label': 'Retention Rate (%)'}, ax=axes[0])\n",
        "    axes[0].set_title('Cohort Retention Analysis')\n",
        "    axes[0].set_ylabel('Cohort Month (First Purchase)')\n",
        "    axes[0].set_xlabel('Months Since First Purchase')\n",
        "\n",
        "    # Plot 2: Revenue per active customer\n",
        "    sns.heatmap(cohort_avg_revenue, annot=True, fmt='.1f', cmap='Blues', linewidths=.5,\n",
        "                cbar_kws={'label': 'Average Revenue'}, ax=axes[1])\n",
        "    axes[1].set_title('Cohort Average Revenue per Active Customer')\n",
        "    axes[1].set_ylabel('Cohort Month (First Purchase)')\n",
        "    axes[1].set_xlabel('Months Since First Purchase')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return retention_matrix, cohort_avg_revenue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPEQ5LNZrdAN"
      },
      "outputs": [],
      "source": [
        "retention_matrix, avg_rev_matrix = cohort_analysis(sales_df_products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLjlDj3NLz4k"
      },
      "source": [
        "## Sales Performance & Trend Analysis\n",
        "\n",
        "`sales_performance_analysis` analyzes overall sales patterns by tracking both daily and monthly revenue trends, helping to identify seasonality and growth. It also highlights the top-performing products in terms of total revenue and quantity sold. By combining time-based trends with product-level insights, this analysis provides a clear view of sales dynamics, customer demand, and key revenue drivers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8N7KWcusqOr"
      },
      "outputs": [],
      "source": [
        "def analyze_sales_trends(sales_df_products):\n",
        "    # Daily and Monthly Sales Trends\n",
        "    daily_sales = sales_df_products.set_index('InvoiceDate').resample('D')['Total_Price'].sum().fillna(0)\n",
        "    monthly_sales = sales_df_products.set_index('InvoiceDate').resample('ME')['Total_Price'].sum().fillna(0)\n",
        "\n",
        "    # Create subplots for sales trends (side by side)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    fig.suptitle('Sales Revenue Trends Over Time', fontsize=16)\n",
        "\n",
        "    # Plot Daily Sales Trend\n",
        "    axes[0].plot(daily_sales.index, daily_sales.values, label='Daily Sales Revenue')\n",
        "    axes[0].set_title('Daily Sales Revenue Over Time')\n",
        "    axes[0].set_xlabel('Date')\n",
        "    axes[0].set_ylabel('Revenue')\n",
        "    axes[0].grid(True)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot Monthly Sales Trend\n",
        "    axes[1].plot(monthly_sales.index, monthly_sales.values, label='Monthly Sales Revenue', marker='o')\n",
        "    axes[1].set_title('Monthly Sales Revenue Over Time (Seasonality)')\n",
        "    axes[1].set_xlabel('Month')\n",
        "    axes[1].set_ylabel('Total_Price')\n",
        "    axes[1].grid(True)\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    # Product Performance\n",
        "    product_performance = sales_df_products.groupby('StockCode').agg(\n",
        "        Total_Quantity_Sold=('Quantity', 'sum'),\n",
        "        Total_Revenue=('Total_Price', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    top_products_revenue = product_performance.sort_values(by='Total_Revenue', ascending=False).head()\n",
        "    top_products_quantity = product_performance.sort_values(by='Total_Quantity_Sold', ascending=False).head()\n",
        "\n",
        "    print(\"\\nTop 5 Products by Revenue:\")\n",
        "    display(top_products_revenue)\n",
        "\n",
        "    print(\"\\nTop 5 Products by Quantity Sold:\")\n",
        "    display(top_products_quantity)\n",
        "\n",
        "    return top_products_revenue, top_products_quantity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2aXREkVxnUY"
      },
      "outputs": [],
      "source": [
        "top_revenue, top_quantity = analyze_sales_trends(sales_df_products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9KQWRRaNrlj"
      },
      "source": [
        "## Returns & Refunds Impact Analysis\n",
        "\n",
        "`returns_refunds_analysis` evaluates the business impact of product returns by quantifying both the total quantity of items returned and the total refund value. It tracks monthly refund trends to reveal fluctuations and patterns in returns over time. Additionally, it highlights the top products driving returns, both by quantity and by refund value, enabling businesses to identify problematic products or categories. This analysis provides insights into return-related revenue loss and helps in improving product quality, customer satisfaction, and inventory management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmcgMyINxnHY"
      },
      "outputs": [],
      "source": [
        "def returns_refunds_analysis(returns_df, date_col='InvoiceDate', quantity_col='Quantity', value_col='Total_Price', product_col='StockCode'):\n",
        "    # Ensure datetime\n",
        "    returns_df[date_col] = pd.to_datetime(returns_df[date_col])\n",
        "\n",
        "    # Summary\n",
        "    total_returns_quantity = returns_df[quantity_col].sum()\n",
        "    total_refund_value = returns_df[value_col].sum()\n",
        "\n",
        "    print(\"\\n--- Returns and Refunds Impact Assessment ---\")\n",
        "    print(f\"Total quantity of items returned: {-total_returns_quantity}\")\n",
        "    print(f\"Total monetary value of refunds: {total_refund_value}\")\n",
        "\n",
        "    #  Monthly return trends\n",
        "    returns_monthly = returns_df.set_index(date_col).resample('ME')[[value_col]].sum().fillna(0)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(returns_monthly.index, returns_monthly[value_col], label='Monthly Refund Value', marker='o', color='red')\n",
        "    plt.title('Monthly Refund Value Over Time')\n",
        "    plt.xlabel('Month')\n",
        "    plt.ylabel('Refund Value')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Top returned products\n",
        "    top_returned_products = returns_df.groupby(product_col).agg(\n",
        "        Total_Quantity_Returned=(quantity_col, 'sum'),\n",
        "        Total_Refund_Value=(value_col, 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Absolute values for ranking\n",
        "    top_returned_products['Abs_Quantity_Returned'] = top_returned_products['Total_Quantity_Returned'].abs()\n",
        "    top_returned_products['Abs_Refund_Value'] = top_returned_products['Total_Refund_Value'].abs()\n",
        "\n",
        "    top_5_returned_by_quantity = top_returned_products.sort_values(by='Abs_Quantity_Returned', ascending=False).head(5)\n",
        "    top_5_returned_by_value = top_returned_products.sort_values(by='Abs_Refund_Value', ascending=False).head(5)\n",
        "\n",
        "    print(\"\\nTop 5 Products by Quantity Returned:\")\n",
        "    display(top_5_returned_by_quantity[[product_col, 'Total_Quantity_Returned', 'Total_Refund_Value']])\n",
        "\n",
        "    print(\"\\nTop 5 Products by Refund Value:\")\n",
        "    display(top_5_returned_by_value[[product_col, 'Total_Quantity_Returned', 'Total_Refund_Value']])\n",
        "\n",
        "    return {\n",
        "        \"total_quantity_returned\": -total_returns_quantity,\n",
        "        \"total_refund_value\": total_refund_value,\n",
        "        \"monthly_trends\": returns_monthly\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QyDIJerPVq8"
      },
      "outputs": [],
      "source": [
        "returns_refunds_analysis(returns_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u53I4yqN3f7"
      },
      "source": [
        "# Task 5\n",
        "\n",
        "Prepare the dataset for clustering by performing advanced customer segmentation. Create a combined dataset that merges RFM metrics with additional segmentation features. Build clustering-ready datasets using multiple approaches such as RFM, Value–Volume, and Temporal Behavioral models. Apply feature scaling to standardize the data for clustering algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJQdEvSkVCN1"
      },
      "source": [
        "## Advanced Customer Segmentation\n",
        "\n",
        "`advanced_customer_segmentation` builds enriched customer profiles by calculating detailed behavioral and transactional metrics. It goes beyond standard RFM by adding features such as **average order value, total items purchased, unique products purchased, average items per order, and average days between purchases**. It also incorporates **return behavior** by computing each customer’s return rate based on refund values.\n",
        "\n",
        "This enriched dataset provides a comprehensive view of customer activity, enabling more effective **clustering, segmentation, and predictive modeling**. Businesses can use it to identify loyal customers, high-return-risk groups, and opportunities for targeted retention or marketing strategies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOxq_FchPeHc"
      },
      "outputs": [],
      "source": [
        "def advanced_customer_segmentation(sales_df_products, returns_df):\n",
        "    # Snapshot date for recency\n",
        "    snapshot_date = sales_df_products['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "\n",
        "    # Aggregate customer-level metrics\n",
        "    customer_level_df = sales_df_products.groupby('Customer ID').agg(\n",
        "        Recency=('InvoiceDate', lambda date: (snapshot_date - date.max()).days),\n",
        "        Frequency=('Invoice', lambda num: num.nunique()),\n",
        "        Monetary=('Total_Price', 'sum'),\n",
        "        Avg_Order_Value=('Total_Price', 'mean'),  # average revenue per transaction\n",
        "        Total_Items_Purchased=('Quantity', 'sum'),\n",
        "        Unique_Products_Purchased=('StockCode', 'nunique')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Average items per order\n",
        "    customer_level_df['Avg_Items_per_Order'] = (\n",
        "        customer_level_df['Total_Items_Purchased'] / customer_level_df['Frequency']\n",
        "    )\n",
        "\n",
        "    # Average days between purchases\n",
        "    customer_invoice_dates = sales_df_products.groupby('Customer ID')['InvoiceDate'].apply(list)\n",
        "    avg_days_between = []\n",
        "    for cid in customer_level_df['Customer ID']:\n",
        "        dates = sorted(customer_invoice_dates[cid])\n",
        "        if len(dates) > 1:\n",
        "            diffs = np.diff(dates)\n",
        "            avg_days_between.append(np.mean(diffs).days)\n",
        "        else:\n",
        "            avg_days_between.append(0)\n",
        "    customer_level_df['Avg_Days_Between_Purchases'] = avg_days_between\n",
        "\n",
        "    # Return rate (monetary)\n",
        "    returns_summary = returns_df.groupby('Customer ID')['Total_Price'].sum().abs().reset_index()\n",
        "    returns_summary.rename(columns={'Total_Price': 'Total_Return_Value'}, inplace=True)\n",
        "\n",
        "    customer_level_df = customer_level_df.merge(returns_summary, on='Customer ID', how='left').fillna(0)\n",
        "    customer_level_df['Return_Rate'] = customer_level_df['Total_Return_Value'] / customer_level_df['Monetary']\n",
        "    customer_level_df['Return_Rate'] = customer_level_df['Return_Rate'].replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    return customer_level_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWrsRqGQPn2a"
      },
      "outputs": [],
      "source": [
        "customer_level_df = advanced_customer_segmentation(sales_df_products, returns_df)\n",
        "display(customer_level_df.head(1))\n",
        "display(rfm_table.head(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baMwJjwOVJQN"
      },
      "source": [
        "## Combined RFM & Segmentation Dataset\n",
        "\n",
        "`combine_rfm_and_segmentation` merges traditional RFM scoring with advanced customer-level segmentation features into a single dataset. This unified view allows analysts to leverage both **transaction recency/frequency/monetary behavior** and **extended engagement metrics** for deeper segmentation, clustering, and customer lifetime value analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRNQ3PFVQxNK"
      },
      "outputs": [],
      "source": [
        "def combine_rfm_and_segmentation(rfm_table, customer_level_df):\n",
        "    # Merge on Customer ID\n",
        "    combined_df = pd.merge(rfm_table, customer_level_df, on=\"Customer ID\", how=\"inner\")\n",
        "    return combined_df\n",
        "\n",
        "combined_df = combine_rfm_and_segmentation(rfm_table, customer_level_df)\n",
        "combined_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "297Mgl0cKeZg"
      },
      "outputs": [],
      "source": [
        "combined_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZRsECEkViel"
      },
      "source": [
        "## Clustering Dataset Preparation\n",
        "\n",
        "`prepare_clustering_df` extracts and organizes the most relevant features from the enriched customer dataset into a **clean, analysis-ready dataframe** for clustering tasks.\n",
        "\n",
        "It ensures a consistent feature set by keeping both **core RFM scores** and **extended engagement metrics** (e.g., order behavior, product diversity, return patterns). This streamlined dataset eliminates noise and aligns customer records for direct use in **unsupervised learning models** like KMeans, GMM, or Hierarchical Clustering.\n",
        "\n",
        "By standardizing the input, this function acts as a **final preprocessing step** before scaling, dimensionality reduction, and clustering, ensuring comparability across customers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPAVtYW4Q8uh"
      },
      "outputs": [],
      "source": [
        "def prepare_clustering_df(df):\n",
        "    # Columns to keep\n",
        "    keep_cols = [\n",
        "        \"Customer ID\",\n",
        "        \"Recency_x\", \"Frequency_x\", \"Monetary_x\", \"R_Score\", \"F_Score\", \"M_Score\",\n",
        "        \"Avg_Order_Value\", \"Total_Items_Purchased\", \"Unique_Products_Purchased\",\n",
        "        \"Avg_Items_per_Order\", \"Avg_Days_Between_Purchases\",\n",
        "        \"Total_Return_Value\", \"Return_Rate\"\n",
        "    ]\n",
        "\n",
        "    clustering_df = df[keep_cols].copy()\n",
        "\n",
        "    # Rename columns\n",
        "    clustering_df.rename(columns={\n",
        "        \"Recency_x\": \"Recency\",\n",
        "        \"Frequency_x\": \"Frequency\",\n",
        "        \"Monetary_x\": \"Monetary\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Convert category/object scores to numeric\n",
        "    for score_col in [\"R_Score\", \"F_Score\", \"M_Score\"]:\n",
        "        clustering_df[score_col] = pd.to_numeric(clustering_df[score_col], errors=\"coerce\")\n",
        "\n",
        "    # Fill NaNs with 0\n",
        "    clustering_df = clustering_df.fillna(0)\n",
        "\n",
        "    # Convert everything except Customer ID to float\n",
        "    for col in clustering_df.columns:\n",
        "        if col != \"Customer ID\":\n",
        "            clustering_df[col] = clustering_df[col].astype(float)\n",
        "\n",
        "    return clustering_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-y0LskvQ_y4"
      },
      "outputs": [],
      "source": [
        "clustering_df = prepare_clustering_df(combined_df)\n",
        "display(clustering_df.head(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T3f8dbNWAmG"
      },
      "source": [
        "## Multi-Dimensional Approaches: RFM, Value–Volume, and Temporal Behavioral Model\n",
        "The customer segmentation framework combines three perspectives:\n",
        "\n",
        "* **RFM (g\\_rfm):** measures recency, frequency, and monetary value to assess engagement and overall customer worth.\n",
        "\n",
        "* **Value & Volume (g\\_pvv):** adds monetary, average order value, total items, and items per order to identify high spenders, bulk buyers, and low-value groups for pricing and loyalty strategies.\n",
        "\n",
        "* **Timing & Habit (g\\_pth):** uses recency, frequency, and average days between purchases to distinguish habitual, occasional, and at-risk buyers, supporting retention and churn prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW5EhW7-RGBf"
      },
      "outputs": [],
      "source": [
        "g_rfm = clustering_df[['Recency', 'Frequency', 'Monetary']].copy() #Customer Segmentation\n",
        "g_pvv = clustering_df[['Monetary', 'Avg_Order_Value', 'Total_Items_Purchased', 'Avg_Items_per_Order']].copy() #Purchase Value Volume\n",
        "g_pth = clustering_df[['Recency', 'Frequency', 'Avg_Days_Between_Purchases']].copy() #Purchase Timing & Habit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfDNdQkZiFD7"
      },
      "outputs": [],
      "source": [
        "def drop_cluster_columns(df):\n",
        "    cluster_cols = [col for col in df.columns if col.endswith('_Cluster')]\n",
        "    df.drop(columns=cluster_cols, inplace=True, errors='ignore')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh61TIbfSCgK"
      },
      "outputs": [],
      "source": [
        "clustering_df = drop_cluster_columns(clustering_df)\n",
        "g_rfm = drop_cluster_columns(g_rfm)\n",
        "g_pvv = drop_cluster_columns(g_pvv)\n",
        "g_pth = drop_cluster_columns(g_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01591c2a"
      },
      "outputs": [],
      "source": [
        "def visualize_rfm_data(g_rfm, g_pvv, g_pth):\n",
        "  # Set the style for the plots\n",
        "  sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "  #  Visualizations for g_rfm (Recency, Frequency, Monetary)\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "  fig.suptitle('Distribution of RFM Features', fontsize=16)\n",
        "\n",
        "  sns.histplot(g_rfm['Recency'], kde=True, ax=axes[0])\n",
        "  axes[0].set_title('Distribution of Recency')\n",
        "  axes[0].set_xlabel('Recency (Days)')\n",
        "  axes[0].set_ylabel('Frequency')\n",
        "\n",
        "  sns.histplot(g_rfm['Frequency'], kde=True, ax=axes[1])\n",
        "  axes[1].set_title('Distribution of Frequency')\n",
        "  axes[1].set_xlabel('Frequency')\n",
        "  axes[1].set_ylabel('Frequency')\n",
        "  axes[1].set_xlim(0, g_rfm['Frequency'].quantile(0.95)) # Limit x-axis for better visualization\n",
        "\n",
        "  sns.histplot(g_rfm['Monetary'], kde=True, ax=axes[2])\n",
        "  axes[2].set_title('Distribution of Monetary')\n",
        "  axes[2].set_xlabel('Monetary Value')\n",
        "  axes[2].set_ylabel('Frequency')\n",
        "  axes[2].set_xlim(0, g_rfm['Monetary'].quantile(0.95))\n",
        "\n",
        "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Visualizations for g_pvv (Purchase Value Volume)\n",
        "  fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "  fig.suptitle('Distribution of Purchase Value/Volume Features', fontsize=16)\n",
        "\n",
        "  sns.histplot(g_pvv['Monetary'], kde=True, ax=axes[0, 0])\n",
        "  axes[0, 0].set_title('Distribution of Monetary Value')\n",
        "  axes[0, 0].set_xlabel('Monetary Value')\n",
        "  axes[0, 0].set_ylabel('Frequency')\n",
        "  axes[0, 0].set_xlim(0, g_pvv['Monetary'].quantile(0.95))\n",
        "\n",
        "  sns.histplot(g_pvv['Avg_Order_Value'], kde=True, ax=axes[0, 1])\n",
        "  axes[0, 1].set_title('Distribution of Average Order Value')\n",
        "  axes[0, 1].set_xlabel('Average Order Value')\n",
        "  axes[0, 1].set_ylabel('Frequency')\n",
        "  axes[0, 1].set_xlim(0, g_pvv['Avg_Order_Value'].quantile(0.95))\n",
        "\n",
        "\n",
        "  sns.histplot(g_pvv['Total_Items_Purchased'], kde=True, ax=axes[1, 0])\n",
        "  axes[1, 0].set_title('Distribution of Total Items Purchased')\n",
        "  axes[1, 0].set_xlabel('Total Items Purchased')\n",
        "  axes[1, 0].set_ylabel('Frequency')\n",
        "  axes[1, 0].set_xlim(0, g_pvv['Total_Items_Purchased'].quantile(0.95))\n",
        "\n",
        "\n",
        "  sns.histplot(g_pvv['Avg_Items_per_Order'], kde=True, ax=axes[1, 1])\n",
        "  axes[1, 1].set_title('Distribution of Average Items per Order')\n",
        "  axes[1, 1].set_xlabel('Average Items per Order')\n",
        "  axes[1, 1].set_ylabel('Frequency')\n",
        "  axes[1, 1].set_xlim(0, g_pvv['Avg_Items_per_Order'].quantile(0.95))\n",
        "\n",
        "\n",
        "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  #  Visualizations for g_pth (Purchase Timing & Habit)\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "  fig.suptitle('Distribution of Purchase Timing & Habit Features', fontsize=16)\n",
        "\n",
        "  sns.histplot(g_pth['Recency'], kde=True, ax=axes[0])\n",
        "  axes[0].set_title('Distribution of Recency')\n",
        "  axes[0].set_xlabel('Recency (Days)')\n",
        "  axes[0].set_ylabel('Frequency')\n",
        "\n",
        "  sns.histplot(g_pth['Frequency'], kde=True, ax=axes[1])\n",
        "  axes[1].set_title('Distribution of Frequency')\n",
        "  axes[1].set_xlabel('Frequency')\n",
        "  axes[1].set_ylabel('Frequency')\n",
        "  axes[1].set_xlim(0, g_pth['Frequency'].quantile(0.95))\n",
        "\n",
        "\n",
        "  sns.histplot(g_pth['Avg_Days_Between_Purchases'], kde=True, ax=axes[2])\n",
        "  axes[2].set_title('Distribution of Average Days Between Purchases')\n",
        "  axes[2].set_xlabel('Average Days Between Purchases')\n",
        "  axes[2].set_ylabel('Frequency')\n",
        "  axes[2].set_xlim(0, g_pth['Avg_Days_Between_Purchases'].quantile(0.95))\n",
        "\n",
        "\n",
        "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "  plt.show()\n",
        "\n",
        "visualize_rfm_data(g_rfm, g_pvv, g_pth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T5CcWFQOtmo"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "`scaling` standardizes selected features with missing value handling, returning a scaled dataframe and the fitted scaler.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7Dmn2FDR3Pl"
      },
      "outputs": [],
      "source": [
        "features = [\n",
        "    'Recency', 'Frequency', 'Monetary', 'R_Score', 'F_Score', 'M_Score', 'Avg_Order_Value',\n",
        "    'Total_Items_Purchased', 'Unique_Products_Purchased',\n",
        "    'Avg_Items_per_Order', 'Avg_Days_Between_Purchases', 'Return_Rate'\n",
        "]\n",
        "features_g_rfm = ['Recency', 'Frequency', 'Monetary']\n",
        "features_g_pvv = ['Monetary', 'Avg_Order_Value', 'Total_Items_Purchased', 'Avg_Items_per_Order']\n",
        "features_g_pth = ['Recency', 'Frequency', 'Avg_Days_Between_Purchases']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVgurjHLhw48"
      },
      "outputs": [],
      "source": [
        "def scaling(df, features, id_col=\"Customer ID\"):\n",
        "    X = df[features].copy()\n",
        "\n",
        "    # Handle missing/infinite values\n",
        "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    X.fillna(X.mean(), inplace=True)\n",
        "\n",
        "    #  Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Return scaled dataframe with ID as index\n",
        "    if id_col in df.columns:\n",
        "        X_scaled_df = pd.DataFrame(X_scaled, columns=features, index=df[id_col])\n",
        "    else:\n",
        "        X_scaled_df = pd.DataFrame(X_scaled, columns=features)\n",
        "    return X_scaled_df, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGOZhMPfiv_8"
      },
      "outputs": [],
      "source": [
        "full_scaled, full_scaler = scaling(clustering_df, features) # Full feature set scaling\n",
        "rfm_scaled, rfm_scaler = scaling(clustering_df, features_g_rfm) # RFM scaling\n",
        "pvv_scaled, pvv_scaler = scaling(clustering_df, features_g_pvv) # Purchase Value & Volume scaling\n",
        "pth_scaled, pth_scaler = scaling(clustering_df, features_g_pth) # Purchase Timing & Habit scaling\n",
        "\n",
        "display(full_scaled.head(1))\n",
        "display(rfm_scaled.head(1))\n",
        "display(pvv_scaled.head(1))\n",
        "display(pth_scaled.head(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNBnDRcbdy-8"
      },
      "source": [
        "# Task 6\n",
        "\n",
        "Apply and compare multiple clustering algorithms including K-Means, Hierarchical, DBSCAN, Gaussian Mixture Models (GMM), Affinity Propagation, MeanShift, Spectral Clustering, HDBSCAN, OPTICS, and Birch. Assign cluster labels to customers and prepare results for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLbFTkTSeG2q"
      },
      "source": [
        "## K-Means Clustering\n",
        "\n",
        "`perform_kmeans_clustering` determines the optimal number of clusters using the **Elbow Method** and **Silhouette Score**, fits a final K-Means model, assigns cluster labels, and visualizes results through **SSE curves, silhouette plots, and heatmaps of cluster profiles**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAZVb4dSVVz-"
      },
      "outputs": [],
      "source": [
        "def perform_kmeans_clustering(X_scaled, df, features, scaler):\n",
        "    # Determine optimal K using Elbow Method and Silhouette Score\n",
        "    sse = []\n",
        "    silhouette_scores = []\n",
        "    k_range = range(2, 11)  # Test K from 2 to 10\n",
        "\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        kmeans.fit(X_scaled)\n",
        "        sse.append(kmeans.inertia_)\n",
        "        score = silhouette_score(X_scaled, kmeans.labels_)\n",
        "        silhouette_scores.append(score)\n",
        "\n",
        "    # Find the elbow point using kneed\n",
        "    knee_locator = KneeLocator(k_range, sse, curve='convex', direction='decreasing')\n",
        "    optimal_k = knee_locator.knee\n",
        "    print(f\"Optimal K (Elbow Method): {optimal_k}\")\n",
        "\n",
        "    # Train final KMeans model with optimal_k\n",
        "    kmeans_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "    kmeans_model.fit(X_scaled)  # Fit the model to assign labels\n",
        "\n",
        "    # Add cluster labels to the original DataFrame\n",
        "    df['KMeans_Cluster'] = kmeans_model.labels_\n",
        "\n",
        "    print(f\"K-Means Clustering with K={optimal_k} complete.\")\n",
        "\n",
        "    # Cluster profiles in original units (not scaled)\n",
        "    cluster_profiles = pd.DataFrame(\n",
        "        scaler.inverse_transform(\n",
        "            pd.DataFrame(\n",
        "                kmeans_model.cluster_centers_,\n",
        "                columns=features\n",
        "            )\n",
        "        ),\n",
        "        columns=features\n",
        "    )\n",
        "    cluster_profiles.index.name = 'Cluster'\n",
        "\n",
        "    # Cluster profiles in scaled space (for heatmap)\n",
        "    cluster_profiles_scaled = pd.DataFrame(kmeans_model.cluster_centers_, columns=features)\n",
        "    cluster_profiles_scaled.index.name = 'Cluster'\n",
        "\n",
        "    # Combined subplot figure\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Elbow Method\n",
        "    axes[0].plot(k_range, sse, marker='o')\n",
        "    axes[0].axvline(optimal_k, color='r', linestyle='--', label=f'Elbow = {optimal_k}')\n",
        "    axes[0].set_title('Elbow Method for Optimal K')\n",
        "    axes[0].set_xlabel('Number of Clusters (K)')\n",
        "    axes[0].set_ylabel('SSE')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Silhouette Score\n",
        "    axes[1].plot(k_range, silhouette_scores, marker='o')\n",
        "    axes[1].set_title('Silhouette Score for K')\n",
        "    axes[1].set_xlabel('Number of Clusters (K)')\n",
        "    axes[1].set_ylabel('Silhouette Score')\n",
        "\n",
        "    # Heatmap of scaled cluster profiles\n",
        "    sns.heatmap(cluster_profiles_scaled.T, annot=True, cmap='viridis', fmt=\".2f\", ax=axes[2])\n",
        "    axes[2].set_title('K-Means Cluster Profiles (Scaled Feature Means)')\n",
        "    axes[2].set_xlabel('Cluster')\n",
        "    axes[2].set_ylabel('Feature')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df, optimal_k, kmeans_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuwBzIE7lX7s"
      },
      "outputs": [],
      "source": [
        "# Call the function to perform clustering\n",
        "clustering_df, optimal_k, kmeans_model = perform_kmeans_clustering(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnI9JyJelX4s"
      },
      "outputs": [],
      "source": [
        "# RFM clustering\n",
        "g_rfm, optimal_k_rfm, kmeans_model_rfm = perform_kmeans_clustering(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSo4DNEOlX0k"
      },
      "outputs": [],
      "source": [
        "# Purchase Value & Volume clustering\n",
        "g_pvv, optimal_k_pvv, kmeans_model_pvv = perform_kmeans_clustering(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr3OiWhylXlV"
      },
      "outputs": [],
      "source": [
        "# Purchase Timing & Habit clustering\n",
        "g_pth, optimal_k_pth, kmeans_model_pth = perform_kmeans_clustering(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Koe3PGC_gxma"
      },
      "outputs": [],
      "source": [
        "def visualize_curr_cluster(X_scaled, df, cluster_col):\n",
        "    # --- Determine palette based on number of clusters ---\n",
        "    n_clusters = df[cluster_col].nunique()\n",
        "    if n_clusters <= 5:\n",
        "        palette = 'viridis'   # continuous\n",
        "        legend_type = 'colorbar'\n",
        "    elif n_clusters <= 10:\n",
        "        palette = 'tab10'     # categorical\n",
        "        legend_type = 'full'\n",
        "    elif n_clusters <= 20:\n",
        "        palette = 'tab20'     # categorical\n",
        "        legend_type = 'full'\n",
        "    else:\n",
        "        palette = sns.color_palette(\"hsv\", n_colors=n_clusters)  # categorical\n",
        "        legend_type = 'full'\n",
        "\n",
        "    # --- PCA ---\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "    pca_df = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])\n",
        "    pca_df[cluster_col] = df[cluster_col].values\n",
        "\n",
        "    # --- t-SNE ---\n",
        "    tsne = TSNE(n_components=2, perplexity=30, random_state=42, max_iter=1000)\n",
        "    X_tsne = tsne.fit_transform(X_scaled)\n",
        "    tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])\n",
        "    tsne_df[cluster_col] = df[cluster_col].values\n",
        "\n",
        "    # --- UMAP ---\n",
        "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "    X_umap = reducer.fit_transform(X_scaled)\n",
        "    umap_df = pd.DataFrame(X_umap, columns=['UMAP1', 'UMAP2'])\n",
        "    umap_df[cluster_col] = df[cluster_col].values\n",
        "\n",
        "    # --- Plotting ---\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for ax, data, x, y, title in zip(\n",
        "        axes,\n",
        "        [pca_df, tsne_df, umap_df],\n",
        "        ['PCA1', 'TSNE1', 'UMAP1'],\n",
        "        ['PCA2', 'TSNE2', 'UMAP2'],\n",
        "        ['Clusters (PCA)', 'Clusters (t-SNE)', 'Clusters (UMAP)']\n",
        "    ):\n",
        "        if palette == 'viridis':\n",
        "            sc = ax.scatter(data[x], data[y], c=data[cluster_col], cmap=palette, s=40, alpha=0.8)\n",
        "            plt.colorbar(sc, ax=ax, label=cluster_col)\n",
        "        else:\n",
        "            sns.scatterplot(\n",
        "                x=x, y=y, hue=cluster_col, data=data,\n",
        "                palette=palette, s=40, alpha=0.8, ax=ax, legend=legend_type\n",
        "            )\n",
        "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "        ax.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3vKzqiFg6w4"
      },
      "outputs": [],
      "source": [
        "visualize_curr_cluster(full_scaled, clustering_df, cluster_col='KMeans_Cluster')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTi5lgdJZilA"
      },
      "outputs": [],
      "source": [
        "visualize_curr_cluster(rfm_scaled, g_rfm, cluster_col='KMeans_Cluster')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-FNulMfg_U4"
      },
      "outputs": [],
      "source": [
        "visualize_curr_cluster(pvv_scaled, g_pvv, cluster_col='KMeans_Cluster')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQl04gORg_KC"
      },
      "outputs": [],
      "source": [
        "visualize_curr_cluster(pth_scaled, g_pth, cluster_col='KMeans_Cluster')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXJr-ktroONr"
      },
      "source": [
        "## Hierarchical Clustering\n",
        "\n",
        "`perform_hierarchical_clustering` performs **Agglomerative Hierarchical Clustering** on a scaled feature set. It estimates the **optimal number of clusters** automatically by analyzing the **largest jump in successive merge distances** in the dendrogram. The function then fits a hierarchical clustering model, assigns cluster labels to the dataset, and generates visual summaries including:\n",
        "\n",
        "* **Dendrogram** with a red dashed line indicating the optimal cluster cut height.\n",
        "* **Heatmap of cluster profiles** showing the mean values of scaled features for each cluster.\n",
        "\n",
        "Cluster profiles can also be **inverse-transformed to original scale** for interpretability. The function returns the updated DataFrame with cluster labels, the fitted hierarchical model, and the number of clusters used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW4qcRMyo76i"
      },
      "outputs": [],
      "source": [
        "def perform_hierarchical_clustering(X_scaled, df, features, scaler):\n",
        "    # Perform hierarchical clustering\n",
        "    Z = linkage(X_scaled, method='ward')\n",
        "\n",
        "    # Find optimal number of clusters automatically\n",
        "    # Distances at each merge\n",
        "    last = Z[-10:, 2]  # last 10 merge distances\n",
        "    last_rev = last[::-1]\n",
        "    idxs = np.arange(1, len(last_rev) + 1)\n",
        "\n",
        "    # Compute differences between successive merge distances\n",
        "    diffs = np.diff(last_rev)\n",
        "\n",
        "    # The largest jump in merge distance\n",
        "    max_gap_index = np.argmax(diffs) + 1\n",
        "    n_clusters_hierarchical = max_gap_index + 1  # +1 because clusters = merges+1\n",
        "\n",
        "    print(f\"Optimal number of clusters (hierarchical) based on max distance jump: {n_clusters_hierarchical}\")\n",
        "\n",
        "    # Choose a number of clusters (for comparison, using optimal_k from KMeans if defined)\n",
        "    n_clusters_to_use = optimal_k  # <-- assuming this is defined globally\n",
        "\n",
        "    hierarchical_model = AgglomerativeClustering(\n",
        "        n_clusters=n_clusters_to_use,\n",
        "        metric='euclidean',\n",
        "        linkage='ward'\n",
        "    )\n",
        "\n",
        "    df['Hierarchical_Cluster'] = hierarchical_model.fit_predict(X_scaled)\n",
        "\n",
        "    print(f\"Hierarchical Clustering with {n_clusters_to_use} clusters complete.\")\n",
        "\n",
        "    # Hierarchical Cluster Profiles (Mean of original scaled features)\n",
        "    hierarchical_profile_scaled = df.groupby('Hierarchical_Cluster')[features].mean()\n",
        "    hierarchical_profile_original = pd.DataFrame(\n",
        "        scaler.inverse_transform(hierarchical_profile_scaled),\n",
        "        columns=features\n",
        "    )\n",
        "\n",
        "    # Dendrogram + Heatmap\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "    # Dendrogram\n",
        "    dendrogram(Z, ax=axes[0])\n",
        "    cut_height = last_rev[max_gap_index]  # height where we cut\n",
        "    axes[0].axhline(y=cut_height, color='red', linestyle='--')\n",
        "    axes[0].set_title('Hierarchical Clustering Dendrogram with Optimal Cut')\n",
        "    axes[0].set_xlabel('Sample Index')\n",
        "    axes[0].set_ylabel('Distance')\n",
        "\n",
        "    # Heatmap\n",
        "    sns.heatmap(hierarchical_profile_scaled.T, annot=True, cmap='viridis', fmt=\".2f\", ax=axes[1])\n",
        "    axes[1].set_title('Hierarchical Cluster Profiles (Scaled Feature Means)')\n",
        "    axes[1].set_xlabel('Cluster')\n",
        "    axes[1].set_ylabel('Feature')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df, hierarchical_model, n_clusters_to_use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK2wY48jsRzV"
      },
      "outputs": [],
      "source": [
        "# Full feature set clustering (Hierarchical)\n",
        "clustering_df, hierarchical_model_full, n_clusters_hier_full = perform_hierarchical_clustering(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCtBeNYzktBX"
      },
      "outputs": [],
      "source": [
        "# RFM clustering (Hierarchical)\n",
        "g_rfm, hierarchical_model_rfm, n_clusters_hier_rfm = perform_hierarchical_clustering(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-XKyFb7lT_H"
      },
      "outputs": [],
      "source": [
        "# Purchase Value & Volume clustering (Hierarchical)\n",
        "g_pvv, hierarchical_model_pvv, n_clusters_hier_pvv = perform_hierarchical_clustering(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6auqcfBlT1Y"
      },
      "outputs": [],
      "source": [
        "# Purchase Timing & Habit clustering (Hierarchical)\n",
        "g_pth, hierarchical_model_pth, n_clusters_hier_pth = perform_hierarchical_clustering(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29Cregzio02-"
      },
      "source": [
        "## DBSCAN Clustering\n",
        "\n",
        "`perform_dbscan_clustering` applies **Density-Based Spatial Clustering (DBSCAN)** to the scaled dataset. It automatically estimates the **optimal `eps` parameter** using the **k-distance graph and elbow method**. The function then fits a DBSCAN model, assigns cluster labels (with `-1` representing noise points), and provides cluster summaries including:\n",
        "\n",
        "* **Number of clusters** detected (excluding noise) and **number of noise points**.\n",
        "* **Cluster profiles** computed as mean values of features for each non-noise cluster, optionally converted back to the original scale for interpretability.\n",
        "\n",
        "This approach is particularly useful for detecting **arbitrarily-shaped clusters** and identifying **outliers or noise** in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqnqIBIaU1kZ"
      },
      "outputs": [],
      "source": [
        "def perform_dbscan_clustering(X_scaled, df, features, scaler, min_samples=5):\n",
        "    # Compute k-distances\n",
        "    neigh = NearestNeighbors(n_neighbors=min_samples)\n",
        "    nbrs = neigh.fit(X_scaled)\n",
        "    distances, indices = nbrs.kneighbors(X_scaled)\n",
        "\n",
        "    # Take the distance to the k-th nearest neighbor\n",
        "    k_distances = np.sort(distances[:, min_samples-1])\n",
        "\n",
        "    # Find elbow (eps)\n",
        "    knee = KneeLocator(range(len(k_distances)), k_distances, curve='convex', direction='increasing')\n",
        "    eps_optimal = k_distances[knee.knee] if knee.knee is not None else np.percentile(k_distances, 90)\n",
        "\n",
        "    print(f\"Optimal eps (using elbow method): {eps_optimal:.4f}, min_samples={min_samples}\")\n",
        "\n",
        "    # Run DBSCAN with optimal eps\n",
        "    dbscan_model = DBSCAN(eps=eps_optimal, min_samples=min_samples)\n",
        "    df['DBSCAN_Cluster'] = dbscan_model.fit_predict(X_scaled)\n",
        "\n",
        "    # Count clusters and noise\n",
        "    n_clusters_dbscan = len(set(df['DBSCAN_Cluster'])) - (1 if -1 in df['DBSCAN_Cluster'] else 0)\n",
        "    n_noise = np.sum(df['DBSCAN_Cluster'] == -1)\n",
        "\n",
        "    print(f\"Number of clusters found: {n_clusters_dbscan}\")\n",
        "    print(f\"Number of noise points: {n_noise}\")\n",
        "\n",
        "    # Cluster profiles (exclude noise)\n",
        "    dbscan_profile = df[df['DBSCAN_Cluster'] != -1].groupby('DBSCAN_Cluster')[features].mean()\n",
        "    if not dbscan_profile.empty:\n",
        "        print(\"\\nCluster Profiles (original scale):\")\n",
        "        dbscan_profile_original = pd.DataFrame(\n",
        "            scaler.inverse_transform(dbscan_profile),\n",
        "            columns=features,\n",
        "            index=dbscan_profile.index\n",
        "        )\n",
        "        display(dbscan_profile_original.head())\n",
        "    else:\n",
        "        print(\"\\nNo meaningful clusters found with current parameters.\")\n",
        "\n",
        "    return df, dbscan_model, n_clusters_dbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76vP7W_IuwfW"
      },
      "outputs": [],
      "source": [
        "clustering_df, dbscan_model, dbscan_clusters = perform_dbscan_clustering(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN6ZQOQVm0qO"
      },
      "outputs": [],
      "source": [
        "# DBSCAN for RFM\n",
        "g_rfm, dbscan_model_rfm, dbscan_clusters_rfm = perform_dbscan_clustering(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdkCONSIm3M8"
      },
      "outputs": [],
      "source": [
        "# DBSCAN for PVV\n",
        "g_pvv, dbscan_model_pvv, dbscan_clusters_pvv = perform_dbscan_clustering(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPRUAAi_m592"
      },
      "outputs": [],
      "source": [
        "# DBSCAN for PTH\n",
        "g_pth, dbscan_model_pth, dbscan_clusters_pth = perform_dbscan_clustering(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjuskVPZqPLk"
      },
      "source": [
        "## Gaussian Mixture Model (GMM) Clustering\n",
        "\n",
        "`perform_gmm_clustering` applies **Gaussian Mixture Model (GMM) clustering** to a scaled dataset. It evaluates multiple numbers of components (clusters) using **BIC (Bayesian Information Criterion)** and **AIC (Akaike Information Criterion)** to determine the optimal number of components. The function then fits a final GMM, assigns cluster labels, and computes cluster profiles including:\n",
        "\n",
        "* **BIC and AIC plots** for each number of components to visualize model selection.\n",
        "* **Optimal number of components** based on minimum BIC score.\n",
        "* **Cluster profiles**, representing the mean values of features for each cluster, optionally inverse-transformed to the original scale for interpretability.\n",
        "\n",
        "GMM clustering is particularly useful for **modeling clusters with elliptical shapes** and **probabilistic cluster assignments**, allowing soft membership of points to clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swQMkyCEvOGO"
      },
      "outputs": [],
      "source": [
        "def perform_gmm_clustering(X_scaled, df, features, scaler):\n",
        "    # Lists to store evaluation metrics\n",
        "    bic_scores = []\n",
        "    aic_scores = []\n",
        "    n_components_range = range(2, 21)\n",
        "\n",
        "    # Fit GMM for different component numbers\n",
        "    for n in n_components_range:\n",
        "        gmm = GaussianMixture(n_components=n, random_state=42, n_init=10)\n",
        "        gmm.fit(X_scaled)\n",
        "        bic_scores.append(gmm.bic(X_scaled))\n",
        "        aic_scores.append(gmm.aic(X_scaled))\n",
        "\n",
        "    # Plot BIC & AIC scores\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(n_components_range, bic_scores, marker='o', color='blue')\n",
        "    plt.title('BIC for GMM')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('BIC Score')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(n_components_range, aic_scores, marker='o', color='green')\n",
        "    plt.title('AIC for GMM')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('AIC Score')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Choose optimal number of components (where BIC or AIC is lowest)\n",
        "    optimal_gmm_components = n_components_range[bic_scores.index(min(bic_scores))]\n",
        "    print(f\"Optimal GMM components (based on BIC): {optimal_gmm_components}\")\n",
        "\n",
        "    # Fit final GMM model\n",
        "    gmm_model = GaussianMixture(n_components=optimal_gmm_components, random_state=42, n_init=10)\n",
        "    # Use the original df to add the cluster labels\n",
        "    df['GMM_Cluster'] = gmm_model.fit_predict(X_scaled)\n",
        "\n",
        "\n",
        "    print(f\"\\nGMM Clustering with {optimal_gmm_components} components complete.\")\n",
        "\n",
        "    # Cluster profiles in original scale\n",
        "    # Need to use the features list and scaler here for meaningful output\n",
        "    gmm_profile_scaled = df.groupby('GMM_Cluster')[features].mean()\n",
        "    gmm_profile_original = pd.DataFrame(\n",
        "        scaler.inverse_transform(gmm_profile_scaled),\n",
        "        columns=features,\n",
        "        index=gmm_profile_scaled.index\n",
        "    )\n",
        "\n",
        "    return df, gmm_model, optimal_gmm_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28Elaklnv4oQ"
      },
      "outputs": [],
      "source": [
        "clustering_df, gmm_model, optimal_gmm_components = perform_gmm_clustering(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13rznwlxhHqv"
      },
      "outputs": [],
      "source": [
        "rfm_df, gmm_model_rfm, optimal_gmm_components_rfm = perform_gmm_clustering(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7Zf_EmNhKbi"
      },
      "outputs": [],
      "source": [
        "pvv_df, gmm_model_pvv, optimal_gmm_components_pvv = perform_gmm_clustering(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR1rVg5HhKXG"
      },
      "outputs": [],
      "source": [
        "pth_df, gmm_model_pth, optimal_gmm_components_pth = perform_gmm_clustering(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTOB3C5tqx6v"
      },
      "source": [
        "## Affinity Propagation Clustering\n",
        "\n",
        "`perform_affinity_propagation` applies **Affinity Propagation (AP)** clustering on a scaled dataset. AP identifies **exemplar points** and forms clusters based on **message passing between points**, without requiring a pre-specified number of clusters. The function:\n",
        "\n",
        "* Fits an **Affinity Propagation model** with user-defined `damping`, `preference`, and `max_iter` parameters.\n",
        "* Assigns **cluster labels** to the dataset in a new column `AP_Cluster`.\n",
        "* Computes **cluster profiles** by averaging feature values for each cluster, both in **scaled and original feature space** for interpretability.\n",
        "* Returns the **number of clusters discovered** automatically by the algorithm.\n",
        "\n",
        "Affinity Propagation is particularly useful when the **number of clusters is unknown** and clusters may vary in size or density.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbZQbWeczKxN"
      },
      "outputs": [],
      "source": [
        "def perform_affinity_propagation(X_scaled, df, features, scaler, damping=0.9, preference=None, max_iter=200):\n",
        "    # Fit Affinity Propagation\n",
        "    ap_model = AffinityPropagation(damping=damping, preference=preference, max_iter=max_iter, random_state=42)\n",
        "    labels = ap_model.fit_predict(X_scaled)\n",
        "\n",
        "    # Add labels to df\n",
        "    df['AP_Cluster'] = labels\n",
        "\n",
        "    n_clusters = len(np.unique(labels))\n",
        "    print(f\"Affinity Propagation found {n_clusters} clusters\")\n",
        "\n",
        "    # Cluster profiles (scaled and original)\n",
        "    ap_profile_scaled = df.groupby('AP_Cluster')[features].mean()\n",
        "    ap_profile_original = pd.DataFrame(\n",
        "        scaler.inverse_transform(ap_profile_scaled),\n",
        "        columns=features,\n",
        "        index=ap_profile_scaled.index\n",
        "    )\n",
        "\n",
        "    return df, ap_model, n_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBKTwYps0Rv8"
      },
      "outputs": [],
      "source": [
        "clustering_df, ap_model, n_clusters = perform_affinity_propagation(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjGGfd--ppis"
      },
      "outputs": [],
      "source": [
        "g_rfm, ap_model_rfm, n_clusters_rfm = perform_affinity_propagation(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r0r8rI7ppYs"
      },
      "outputs": [],
      "source": [
        "g_pvv, ap_model_pvv, n_clusters_pvv = perform_affinity_propagation(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4lnqtv0ppSL"
      },
      "outputs": [],
      "source": [
        "g_pth, ap_model_pth, n_clusters_pth = perform_affinity_propagation(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epEKtR7qsEuI"
      },
      "source": [
        "## MeanShift Clustering\n",
        "\n",
        "`perform_meanshift` applies **MeanShift clustering** on a scaled dataset. MeanShift identifies clusters by **iteratively shifting points towards regions of high density**, without requiring a predefined number of clusters. The function:\n",
        "\n",
        "* Estimates an appropriate **bandwidth** automatically using `estimate_bandwidth`.\n",
        "* Fits a **MeanShift model** and assigns cluster labels to the dataset in a new column `MS_Cluster`.\n",
        "* Determines the **number of clusters found** and prints it.\n",
        "* Computes **cluster profiles** as mean values of features for each cluster, transformed back to the **original feature scale** for interpretability.\n",
        "\n",
        "MeanShift is particularly useful for **discovering clusters of arbitrary shape** and automatically detecting the number of clusters based on **data density**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aan4BLK62F5"
      },
      "outputs": [],
      "source": [
        "def perform_meanshift(X_scaled, df, features, scaler):\n",
        "    bandwidth = estimate_bandwidth(X_scaled, quantile=0.2, n_samples=500)\n",
        "    ms_model = MeanShift(bandwidth=bandwidth)\n",
        "    clusters = ms_model.fit_predict(X_scaled)\n",
        "    df['MS_Cluster'] = clusters\n",
        "\n",
        "    n_clusters = len(set(clusters))\n",
        "    print(f\"Number of clusters found by MeanShift: {n_clusters}\")\n",
        "\n",
        "    profiles = df.groupby('MS_Cluster')[features].mean()\n",
        "    profiles = pd.DataFrame(scaler.inverse_transform(profiles), index=profiles.index, columns=features)\n",
        "\n",
        "    return df, ms_model, n_clusters, profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xi0_yz-65Uc"
      },
      "outputs": [],
      "source": [
        "clustering_df, ms_model, n_clusters_ms, profiles_ms = perform_meanshift(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk5zdR3-4u37"
      },
      "outputs": [],
      "source": [
        "g_rfm, ms_model_rfm, n_clusters_ms_rfm, profiles_ms_rfm = perform_meanshift(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEbjyBo94xKW"
      },
      "outputs": [],
      "source": [
        "g_pvv, ms_model_pvv, n_clusters_ms_pvv, profiles_ms_pvv = perform_meanshift(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjOhlpVK4zoX"
      },
      "outputs": [],
      "source": [
        "g_pth, ms_model_pth, n_clusters_ms_pth, profiles_ms_pth = perform_meanshift(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMrzORkpsLgD"
      },
      "source": [
        "## Spectral Clustering\n",
        "\n",
        "`perform_spectral` applies **Spectral Clustering** on a scaled dataset. This method uses the **eigenvalues of a similarity matrix** to reduce the data to a lower-dimensional space before applying a clustering algorithm, typically K-Means. The function:\n",
        "\n",
        "* Fits a **Spectral Clustering model** with a specified number of clusters (`n_clusters`) and assigns cluster labels in a new column `SP_Cluster`.\n",
        "* Prints the **number of clusters found**, which is predefined by the user.\n",
        "* Computes **cluster profiles** by averaging feature values for each cluster, and transforms them back to the **original feature scale** for interpretability.\n",
        "\n",
        "Spectral Clustering is particularly effective for **detecting clusters with complex, non-convex shapes** and when the data is **not well separated in the original feature space**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEhieZTH65Qu"
      },
      "outputs": [],
      "source": [
        "def perform_spectral(X_scaled, df, features, scaler, n_clusters=3):\n",
        "    sp_model = SpectralClustering(n_clusters=n_clusters, assign_labels=\"kmeans\", random_state=42)\n",
        "    clusters = sp_model.fit_predict(X_scaled)\n",
        "    df['SP_Cluster'] = clusters\n",
        "\n",
        "    print(f\"Number of clusters found by Spectral Clustering: {n_clusters}\")\n",
        "\n",
        "    profiles = df.groupby('SP_Cluster')[features].mean()\n",
        "    profiles = pd.DataFrame(scaler.inverse_transform(profiles), index=profiles.index, columns=features)\n",
        "\n",
        "    return df, sp_model, n_clusters, profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhCOfALC65NL"
      },
      "outputs": [],
      "source": [
        "clustering_df, sp_model, n_clusters_sp, profiles_sp = perform_spectral(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk000tYs-Dfb"
      },
      "outputs": [],
      "source": [
        "# For RFM dataset\n",
        "g_rfm, sp_model_rfm, n_clusters_sp_rfm, profiles_sp_rfm = perform_spectral(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-11Wj3f-Fq7"
      },
      "outputs": [],
      "source": [
        "# For PVV dataset\n",
        "g_pvv, sp_model_pvv, n_clusters_sp_pvv, profiles_sp_pvv = perform_spectral(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpqjMZO4-Ffl"
      },
      "outputs": [],
      "source": [
        "# For PTH dataset\n",
        "g_pth, sp_model_pth, n_clusters_sp_pth, profiles_sp_pth = perform_spectral(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS3gqkVzskol"
      },
      "source": [
        "## HDBSCAN Clustering\n",
        "\n",
        "`perform_hdbscan` applies **HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)** on a scaled dataset. HDBSCAN extends DBSCAN by **handling clusters of varying densities** and **identifying noise points** automatically. The function:\n",
        "\n",
        "* Fits an **HDBSCAN model** with a specified `min_cluster_size` and assigns cluster labels to the dataset in a new column `HDB_Cluster`.\n",
        "* Computes the **number of clusters found** (excluding noise) and the **number of noise points**.\n",
        "* Calculates **cluster profiles** as mean feature values for each cluster, ignoring noise points, and transforms them back to the **original feature scale** for interpretability.\n",
        "* If no meaningful clusters are found, it reports this and returns an empty profile DataFrame.\n",
        "\n",
        "HDBSCAN is particularly useful for **discovering clusters of varying shapes and densities** and robustly identifying **outliers** in complex datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB79rFEl65Jd"
      },
      "outputs": [],
      "source": [
        "def perform_hdbscan(X_scaled, df, features, scaler, min_cluster_size=5):\n",
        "    hdb_model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
        "    clusters = hdb_model.fit_predict(X_scaled)\n",
        "    df['HDB_Cluster'] = clusters\n",
        "\n",
        "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
        "    n_noise = np.sum(clusters == -1)\n",
        "    print(f\"Number of clusters found by HDBSCAN: {n_clusters}\")\n",
        "    print(f\"Number of noise points: {n_noise}\")\n",
        "\n",
        "    profiles = df[df['HDB_Cluster'] != -1].groupby('HDB_Cluster')[features].mean()\n",
        "    if not profiles.empty:\n",
        "        profiles = pd.DataFrame(scaler.inverse_transform(profiles), index=profiles.index, columns=features)\n",
        "    else:\n",
        "        profiles = pd.DataFrame()\n",
        "        print(\"\\nNo meaningful clusters found with current parameters.\")\n",
        "\n",
        "    return df, hdb_model, n_clusters, profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2SI-UZH65E4"
      },
      "outputs": [],
      "source": [
        "clustering_df, hdb_model, n_clusters_hdb, profiles_hdb = perform_hdbscan(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5qJ4Qtl_Bx6"
      },
      "outputs": [],
      "source": [
        "# For RFM dataset\n",
        "g_rfm, hdb_model_rfm, n_clusters_hdb_rfm, profiles_hdb_rfm = perform_hdbscan(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptD1J7ky_Bnk"
      },
      "outputs": [],
      "source": [
        "# For PVV dataset\n",
        "g_pvv, hdb_model_pvv, n_clusters_hdb_pvv, profiles_hdb_pvv = perform_hdbscan(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBoSEhat_BaO"
      },
      "outputs": [],
      "source": [
        "# For PTH dataset\n",
        "g_pth, hdb_model_pth, n_clusters_hdb_pth, profiles_hdb_pth = perform_hdbscan(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSPAagyNs1FT"
      },
      "source": [
        "## OPTICS Clustering\n",
        "\n",
        "`perform_optics` applies **OPTICS (Ordering Points To Identify the Clustering Structure)** on a scaled dataset. OPTICS is a **density-based clustering algorithm** similar to DBSCAN, but it can **handle varying cluster densities** without requiring a global `eps` parameter. The function:\n",
        "\n",
        "* Fits an **OPTICS model** with user-defined `min_samples`, `xi`, and `min_cluster_size` parameters.\n",
        "* Assigns cluster labels to the dataset in a new column `OPTICS_Cluster`, with `-1` indicating noise points.\n",
        "* Computes the **number of clusters found** (excluding noise) and the **number of noise points**.\n",
        "* Generates **cluster profiles** by averaging feature values for each non-noise cluster, and transforms them back to the **original feature scale** for interpretability.\n",
        "* If no meaningful clusters are detected, it reports this and returns an empty profile DataFrame.\n",
        "\n",
        "OPTICS is particularly useful for **discovering clusters of varying shapes and densities**, while also **robustly detecting outliers** in complex datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFyIE3Iz646u"
      },
      "outputs": [],
      "source": [
        "def perform_optics(X_scaled, df, features, scaler, min_samples=5, xi=0.05, min_cluster_size=0.1):\n",
        "    optics_model = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)\n",
        "    clusters = optics_model.fit_predict(X_scaled)\n",
        "    df['OPTICS_Cluster'] = clusters\n",
        "\n",
        "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
        "    n_noise = np.sum(clusters == -1)\n",
        "    print(f\"Number of clusters found by OPTICS: {n_clusters}\")\n",
        "    print(f\"Number of noise points: {n_noise}\")\n",
        "\n",
        "    profiles = df[df['OPTICS_Cluster'] != -1].groupby('OPTICS_Cluster')[features].mean()\n",
        "    if not profiles.empty:\n",
        "        profiles = pd.DataFrame(scaler.inverse_transform(profiles), index=profiles.index, columns=features)\n",
        "    else:\n",
        "        profiles = pd.DataFrame()\n",
        "        print(\"\\nNo meaningful clusters found with current parameters.\")\n",
        "\n",
        "    return df, optics_model, n_clusters, profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4lrJyrV7JJL"
      },
      "outputs": [],
      "source": [
        "clustering_df, optics_model, n_clusters_opt, profiles_opt = perform_optics(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlICZXRtAUvj"
      },
      "outputs": [],
      "source": [
        "# For RFM dataset\n",
        "g_rfm, optics_model_rfm, n_clusters_opt_rfm, profiles_opt_rfm = perform_optics(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SPLgfbCAUrd"
      },
      "outputs": [],
      "source": [
        "# For PVV dataset\n",
        "g_pvv, optics_model_pvv, n_clusters_opt_pvv, profiles_opt_pvv = perform_optics(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VwkaQKOAUmy"
      },
      "outputs": [],
      "source": [
        "# For PTH dataset\n",
        "g_pth, optics_model_pth, n_clusters_opt_pth, profiles_opt_pth = perform_optics(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzvBDilDs7_O"
      },
      "source": [
        "## Birch Clustering\n",
        "\n",
        "`perform_birch` applies **Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)** on a scaled dataset. BIRCH is designed for **large datasets** and incrementally builds a **clustering feature tree** to summarize the data before applying global clustering. The function:\n",
        "\n",
        "* Fits a **BIRCH model** with specified `n_clusters`, `threshold`, and `branching_factor`.\n",
        "* Assigns cluster labels to the dataset in a new column `Birch_Cluster`.\n",
        "* Computes the **number of clusters found**.\n",
        "* Generates **cluster profiles** by averaging feature values for each cluster and transforms them back to the **original feature scale** for interpretability.\n",
        "\n",
        "BIRCH is particularly useful for **large-scale datasets**, allowing **efficient and incremental clustering** with good scalability and memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB7bN21p7JFz"
      },
      "outputs": [],
      "source": [
        "def perform_birch(X_scaled, df, features, scaler, n_clusters=3, threshold=0.5, branching_factor=50):\n",
        "    birch_model = Birch(n_clusters=n_clusters, threshold=threshold, branching_factor=branching_factor)\n",
        "    clusters = birch_model.fit_predict(X_scaled)\n",
        "    df['Birch_Cluster'] = clusters\n",
        "\n",
        "    n_clusters = len(set(clusters))\n",
        "    print(f\"Number of clusters found by Birch: {n_clusters}\")\n",
        "\n",
        "    profiles = df.groupby('Birch_Cluster')[features].mean()\n",
        "    profiles = pd.DataFrame(scaler.inverse_transform(profiles), index=profiles.index, columns=features)\n",
        "\n",
        "    return df, birch_model, n_clusters, profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDRcdSLN7JCO"
      },
      "outputs": [],
      "source": [
        "clustering_df, birch_model, n_clusters_birch, profiles_birch = perform_birch(full_scaled, clustering_df.copy(), features, full_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLnSFO23A-dc"
      },
      "outputs": [],
      "source": [
        "# For RFM dataset\n",
        "g_rfm, birch_model_rfm, n_clusters_birch_rfm, profiles_birch_rfm = perform_birch(rfm_scaled, g_rfm.copy(), features_g_rfm, rfm_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFbCMiFyA-Y1"
      },
      "outputs": [],
      "source": [
        "# For PVV dataset\n",
        "g_pvv, birch_model_pvv, n_clusters_birch_pvv, profiles_birch_pvv = perform_birch(pvv_scaled, g_pvv.copy(), features_g_pvv, pvv_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCihOcaVA-Ut"
      },
      "outputs": [],
      "source": [
        "# For PTH dataset\n",
        "g_pth, birch_model_pth, n_clusters_birch_pth, profiles_birch_pth = perform_birch(pth_scaled, g_pth.copy(), features_g_pth, pth_scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQE_ZpT-tIS6"
      },
      "source": [
        "# Task 7\n",
        "\n",
        "Visualize clustering results using different dimensionality reduction and visualization techniques. Apply PCA for cluster visualization in reduced dimensions. Create feature-based visualizations without PCA. Use t-SNE and UMAP for advanced non-linear visualizations to better interpret cluster structures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UegiriatNWJ"
      },
      "source": [
        "## PCA-Based Cluster Visualization\n",
        "\n",
        "`visualize_clusters_pca` provides a **2D visualization of clustering results** by projecting the scaled dataset onto the first two **principal components (PCA)**. The function:\n",
        "\n",
        "* Automatically detects all **cluster columns** in the DataFrame ending with `_Cluster`.\n",
        "* Handles **noise-based clustering methods** such as DBSCAN, HDBSCAN, and OPTICS, displaying noise points (`-1`) in gray.\n",
        "* Plots **scatterplots of PC1 vs PC2** for each cluster column, using a **distinct color palette** for each cluster.\n",
        "* Supports **multiple clustering results simultaneously**, arranging up to three plots per row and removing unused subplot axes.\n",
        "* Enhances interpretability by showing **cluster separations** in reduced 2D space, making it easy to compare different clustering algorithms visually.\n",
        "\n",
        "This function is particularly useful for **quickly inspecting cluster structure**, identifying overlapping clusters, and evaluating **how well the clustering separates the data** in a low-dimensional representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YE-W3wpBUlR"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters_pca(X_scaled, df):\n",
        "    # PCA transformation\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "    X_pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=df.index)\n",
        "\n",
        "    # Automatically extract cluster columns (ending with \"_Cluster\")\n",
        "    cluster_columns = [col for col in df.columns if col.endswith(\"_Cluster\")]\n",
        "\n",
        "    if not cluster_columns:\n",
        "        raise ValueError(\"No cluster columns found in the dataframe ending with '_Cluster'\")\n",
        "    available_cols = [col for col in cluster_columns if col in df.columns]\n",
        "    for col in available_cols:\n",
        "        X_pca_df[col] = df[col]\n",
        "\n",
        "    # Plot settings\n",
        "    n_plots = len(available_cols)\n",
        "    n_rows = (n_plots + 2) // 3  # up to 3 per row\n",
        "    fig, axes = plt.subplots(n_rows, 3, figsize=(18, 5 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for ax, col in zip(axes, available_cols):\n",
        "        unique_clusters = sorted(X_pca_df[col].unique())\n",
        "        n_clusters = len(unique_clusters)\n",
        "\n",
        "        # Handle noise (cluster = -1)\n",
        "        if -1 in unique_clusters:\n",
        "            colors = [(0.6, 0.6, 0.6)] + sns.color_palette(\"viridis\", n_clusters - 1)\n",
        "            palette = dict(zip(unique_clusters, colors))\n",
        "        else:\n",
        "            palette = sns.color_palette(\"viridis\", n_clusters)\n",
        "\n",
        "        sns.scatterplot(\n",
        "            data=X_pca_df, x='PC1', y='PC2', hue=col,\n",
        "            palette=palette, s=70, alpha=0.85,\n",
        "            edgecolor='black', linewidth=0.3, ax=ax\n",
        "        )\n",
        "        ax.set_title(f\"{col.replace('_Cluster','')} Clusters\", fontsize=14, fontweight='bold')\n",
        "        ax.legend().remove()\n",
        "\n",
        "    # Remove unused subplots if any\n",
        "    for i in range(len(available_cols), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRmYoq5LBZap"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_pca(full_scaled, clustering_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYbdxd7k0DgO"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_pca(rfm_scaled, g_rfm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6cSLJsLFyMC"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_pca(pvv_scaled, g_pvv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4946uxN1FyIF"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_pca(pth_scaled, g_pth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyEbGSMwtcfH"
      },
      "source": [
        "## Feature-Based Cluster Visualization (No PCA)\n",
        "\n",
        "`visualize_clusters_feature_based` provides a **2D visualization of clustering results using two actual features** from the dataset, without applying PCA. The function:\n",
        "\n",
        "* Automatically detects all **cluster columns** in the DataFrame ending with `_Cluster`.\n",
        "* Handles **noise-based clustering methods** such as DBSCAN, HDBSCAN, and OPTICS, showing noise points (`-1`) in gray.\n",
        "* Plots **scatterplots of the two selected features** for each cluster column, using distinct colors for each cluster.\n",
        "* Supports **multiple clustering results simultaneously**, arranging up to three plots per row and removing unused subplot axes.\n",
        "* Enhances interpretability by allowing **direct inspection of cluster structure in the original feature space**, helping to understand how clusters relate to actual feature values.\n",
        "\n",
        "This function is particularly useful for **comparing clustering results on real features** and evaluating cluster separation without dimensionality reduction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAIQpftBmCWu"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters_feature_based(df, features):\n",
        "    # Choose two features for visualization\n",
        "    f1, f2 = features[0], features[1]\n",
        "\n",
        "    # Automatically extract cluster columns (ending with \"_Cluster\")\n",
        "    cluster_columns = [col for col in df.columns if col.endswith(\"_Cluster\")]\n",
        "\n",
        "    if not cluster_columns:\n",
        "        raise ValueError(\"No cluster columns found in the dataframe ending with '_Cluster'\")\n",
        "\n",
        "    available_cols = [col for col in cluster_columns if col in df.columns]\n",
        "    n_plots = len(available_cols)\n",
        "    n_rows = (n_plots + 2) // 3  # up to 3 per row\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, 3, figsize=(18, 5 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for ax, col in zip(axes, available_cols):\n",
        "        unique_clusters = sorted(df[col].unique())\n",
        "        n_clusters = len(unique_clusters)\n",
        "\n",
        "        # Handle noise (-1)\n",
        "        if -1 in unique_clusters:\n",
        "            colors = [(0.6, 0.6, 0.6)] + sns.color_palette(\"viridis\", n_clusters - 1)\n",
        "            palette = dict(zip(unique_clusters, colors))\n",
        "        else:\n",
        "            palette = sns.color_palette(\"viridis\", n_clusters)\n",
        "\n",
        "        sns.scatterplot(\n",
        "            data=df, x=f1, y=f2, hue=col,\n",
        "            palette=palette, s=70, alpha=0.85,\n",
        "            edgecolor='black', linewidth=0.3, ax=ax\n",
        "        )\n",
        "        ax.set_title(f\"{col.replace('_Cluster','')} Clusters\", fontsize=14, fontweight='bold')\n",
        "        ax.legend().remove()\n",
        "\n",
        "    # Remove unused subplots if any\n",
        "    for i in range(len(available_cols), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGl6l34t85L7"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_feature_based(clustering_df, features=[\"Recency\", \"Frequency\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPu7YU2wG1Dl"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_feature_based(g_rfm, [\"Recency\", \"Frequency\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZakHdHTG09c"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_feature_based(g_pvv, [\"Monetary\", \"Total_Items_Purchased\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqJWkyOFG0Z0"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_feature_based(g_pth, [\"Recency\", \"Avg_Days_Between_Purchases\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abZu71ee9Efc"
      },
      "source": [
        "## t-SNE-Based Cluster Visualization\n",
        "\n",
        "`visualize_clusters_tsne` provides a **2D visualization of clustering results using t-SNE** on the scaled feature set. The function:\n",
        "\n",
        "* Automatically detects all **cluster columns** in the DataFrame ending with `_Cluster`.\n",
        "* Applies **t-SNE** to reduce the high-dimensional scaled features to two dimensions (`TSNE1` and `TSNE2`) for visualization.\n",
        "* Plots **scatterplots for each cluster column** in a grid layout, using a distinct color palette (`tab10`) for clusters.\n",
        "* Supports **multiple clustering results simultaneously**, arranging subplots in rows and columns for better readability.\n",
        "* Enhances interpretability by showing **non-linear relationships** and **cluster separations** in a low-dimensional t-SNE projection.\n",
        "\n",
        "This function is particularly useful for **visualizing complex cluster structures**, especially when clusters are not linearly separable in the original feature space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rs2QCuUAGmD"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters_tsne(df, X_scaled):\n",
        "    # Automatically extract cluster columns (ending with \"_Cluster\")\n",
        "    cluster_columns = [col for col in df.columns if col.endswith(\"_Cluster\")]\n",
        "\n",
        "    if not cluster_columns:\n",
        "        raise ValueError(\"No cluster columns found in the dataframe ending with '_Cluster'\")\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate=200)\n",
        "    X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "    tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'], index=df.index)\n",
        "\n",
        "    # Merge with clustering results\n",
        "    tsne_df = pd.concat([tsne_df, df[cluster_columns]], axis=1)\n",
        "\n",
        "    # Plot all clusters in a grid\n",
        "    n_clusters = len(cluster_columns)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_clusters + n_cols - 1) // n_cols\n",
        "\n",
        "    plt.figure(figsize=(6*n_cols, 5*n_rows))\n",
        "\n",
        "    for i, col in enumerate(cluster_columns, 1):\n",
        "        ax = plt.subplot(n_rows, n_cols, i)\n",
        "        sns.scatterplot(\n",
        "            data=tsne_df,\n",
        "            x='TSNE1', y='TSNE2',\n",
        "            hue=col,\n",
        "            palette='tab10',\n",
        "            s=40, alpha=0.8, edgecolor=None,\n",
        "            legend=False\n",
        "        )\n",
        "        ax.set_title(f\"t-SNE: {col}\")\n",
        "\n",
        "    # Increase spacing between subplots\n",
        "    plt.subplots_adjust(wspace=0.25, hspace=0.35)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNDMaGL8OAn4"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_tsne(clustering_df, full_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwOt6NJwJLZ5"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_tsne(g_rfm, rfm_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlfQRqNmJLGm"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_tsne(g_pvv, pvv_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfWkXcovJLCU"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_tsne(g_pth, pth_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld7196Euucjw"
      },
      "source": [
        "## UMAP-Based Cluster Visualization\n",
        "\n",
        "`visualize_clusters_umap` provides a **2D visualization of clustering results using UMAP** on the scaled feature set. The function:\n",
        "\n",
        "* Automatically detects all **cluster columns** in the DataFrame ending with `_Cluster`.\n",
        "* Applies **UMAP** to reduce the high-dimensional scaled features to two dimensions (`UMAP1` and `UMAP2`) while preserving **local and global data structure**.\n",
        "* Plots **scatterplots for each cluster column** in a grid layout, using a distinct color palette (`tab10`) for clusters.\n",
        "* Supports **multiple clustering results simultaneously**, arranging subplots in rows and columns for readability.\n",
        "* Enhances interpretability by showing **cluster separations and relationships** in a low-dimensional UMAP projection, which is particularly effective for **non-linear structures** in the data.\n",
        "\n",
        "This function is especially useful for **visualizing complex, high-dimensional clusters** while maintaining both **local neighborhood relationships** and overall cluster topology."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKvAAKImU9Vy"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters_umap(df, X_scaled):\n",
        "    # Automatically extract cluster columns (ending with \"_Cluster\")\n",
        "    cluster_columns = [col for col in df.columns if col.endswith(\"_Cluster\")]\n",
        "\n",
        "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "    X_umap = reducer.fit_transform(X_scaled)\n",
        "\n",
        "    umap_df = pd.DataFrame(X_umap, columns=['UMAP1', 'UMAP2'], index=df.index)\n",
        "\n",
        "    # Merge with clustering results\n",
        "    umap_df = pd.concat([umap_df, df[cluster_columns]], axis=1)\n",
        "\n",
        "    # Plot all clusters in a grid\n",
        "    n_clusters = len(cluster_columns)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_clusters + n_cols - 1) // n_cols\n",
        "\n",
        "    fig = plt.figure(figsize=(6*n_cols, 5*n_rows))\n",
        "\n",
        "    for i, col in enumerate(cluster_columns, 1):\n",
        "        ax = plt.subplot(n_rows, n_cols, i)\n",
        "        sns.scatterplot(\n",
        "            data=umap_df,\n",
        "            x='UMAP1', y='UMAP2',\n",
        "            hue=col,\n",
        "            palette='tab10',\n",
        "            s=40, alpha=0.8, edgecolor=None,\n",
        "            legend=False\n",
        "        )\n",
        "        ax.set_title(f\"UMAP: {col}\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "    # Increase spacing between subplots\n",
        "    plt.subplots_adjust(wspace=0.3, hspace=0.4)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvzrpMBUVB4r"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_umap(clustering_df, full_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUOVe9IGQ8cO"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_umap(g_rfm, rfm_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwg6gvfjQ8Xg"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_umap(g_pvv, pvv_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5GnvWgqQ8TO"
      },
      "outputs": [],
      "source": [
        "visualize_clusters_umap(g_pth, pth_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDwLh0UeuzB_"
      },
      "source": [
        "# Task 8\n",
        "\n",
        "Evaluate the performance of clustering algorithms using metrics such as Silhouette Score, Calinski–Harabasz Index, and Davies–Bouldin Index. Summarize the results in a comparison table and interpret which clustering method provides the most meaningful segmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMr_m-ntu-K5"
      },
      "source": [
        "## Clustering Evaluation Metrics Table\n",
        "\n",
        "`clustering_metrics_table` generates a **comparative table of clustering evaluation metrics** for all cluster assignments in a dataset. The function:\n",
        "\n",
        "* Automatically detects all **cluster columns** in the DataFrame ending with `_Cluster`.\n",
        "* Computes three commonly used clustering metrics for each algorithm:\n",
        "\n",
        "  * **Silhouette Score** – measures how similar points are within a cluster compared to other clusters (higher is better).\n",
        "  * **Calinski-Harabasz Index** – evaluates cluster dispersion; higher values indicate well-separated clusters.\n",
        "  * **Davies-Bouldin Index** – evaluates cluster similarity; lower values indicate better clustering.\n",
        "* Handles **noise-based clustering algorithms** like DBSCAN, HDBSCAN, and OPTICS, where all points might be noise (`-1`), marking metrics as `\"N/A\"` if computation is not possible.\n",
        "* Returns a **DataFrame summarizing metrics for each clustering algorithm**, making it easy to compare cluster quality across different methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIojvMUxW4LH"
      },
      "outputs": [],
      "source": [
        "def clustering_metrics_table(X_scaled, df):\n",
        "    # Automatically extract cluster columns (ending with \"_Cluster\")\n",
        "    cluster_columns = [col for col in df.columns if col.endswith(\"_Cluster\")]\n",
        "\n",
        "    metrics = []\n",
        "\n",
        "    for col in cluster_columns:\n",
        "        labels = df[col].values\n",
        "\n",
        "        # Skip if clustering failed (e.g., all -1 in DBSCAN/HDBSCAN)\n",
        "        if len(set(labels)) <= 1 or (len(set(labels)) == 2 and -1 in labels):\n",
        "            metrics.append({\n",
        "                \"Algorithm\": col,\n",
        "                \"Silhouette\": \"N/A\",\n",
        "                \"Calinski-Harabasz\": \"N/A\",\n",
        "                \"Davies-Bouldin\": \"N/A\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            silhouette = silhouette_score(X_scaled, labels)\n",
        "            calinski = calinski_harabasz_score(X_scaled, labels)\n",
        "            davies = davies_bouldin_score(X_scaled, labels)\n",
        "        except Exception as e:\n",
        "            silhouette, calinski, davies = \"Err\", \"Err\", \"Err\"\n",
        "\n",
        "        metrics.append({\n",
        "            \"Algorithm\": col,\n",
        "            \"Silhouette\": round(silhouette, 4) if isinstance(silhouette, float) else silhouette,\n",
        "            \"Calinski-Harabasz\": round(calinski, 2) if isinstance(calinski, float) else calinski,\n",
        "            \"Davies-Bouldin\": round(davies, 4) if isinstance(davies, float) else davies\n",
        "        })\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "    return metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R52Kij22XAoH"
      },
      "outputs": [],
      "source": [
        "metrics_clustering_df = clustering_metrics_table(full_scaled, clustering_df)\n",
        "print(\"\\n--- Clustering Evaluation Metrics ---\\n\")\n",
        "display(metrics_clustering_df.style.set_table_styles(\n",
        "    [{'selector': 'th', 'props': [('font-size', '12pt'), ('text-align', 'center')]},\n",
        "     {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAfhhQTAYIKE"
      },
      "outputs": [],
      "source": [
        "metrics_g_rfm = clustering_metrics_table(rfm_scaled, g_rfm)\n",
        "print(\"\\n--- Clustering Evaluation Metrics ---\\n\")\n",
        "display(metrics_g_rfm.style.set_table_styles(\n",
        "    [{'selector': 'th', 'props': [('font-size', '12pt'), ('text-align', 'center')]},\n",
        "     {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJo3wxG1YIDo"
      },
      "outputs": [],
      "source": [
        "metrics_g_pvv = clustering_metrics_table(full_scaled, g_pvv)\n",
        "print(\"\\n--- Clustering Evaluation Metrics ---\\n\")\n",
        "display(metrics_g_pvv.style.set_table_styles(\n",
        "    [{'selector': 'th', 'props': [('font-size', '12pt'), ('text-align', 'center')]},\n",
        "     {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyJXEcs3YH9O"
      },
      "outputs": [],
      "source": [
        "metrics_g_pth = clustering_metrics_table(full_scaled, g_pth)\n",
        "print(\"\\n--- Clustering Evaluation Metrics ---\\n\")\n",
        "display(metrics_g_pth.style.set_table_styles(\n",
        "    [{'selector': 'th', 'props': [('font-size', '12pt'), ('text-align', 'center')]},\n",
        "     {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7AdFoopH4A5"
      },
      "outputs": [],
      "source": [
        "clustering_df.to_csv('clustering_df.csv', index=False)\n",
        "g_rfm.to_csv('g_rfm.csv', index=False)\n",
        "g_pvv.to_csv('g_pvv.csv', index=False)\n",
        "g_pth.to_csv('g_pth.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}